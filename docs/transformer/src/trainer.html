<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>transformer.src.trainer API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>transformer.src.trainer</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import sys
from typing import List, Tuple

import torch
import torch.nn as nn
import tokenizers
import matplotlib.pyplot as plt
from evaluate import load as load_metric
from src.seq2seq_transformer import Seq2SeqTransformer
from utils.config import TrainerConfig
from src.processor import Processor
from time import perf_counter
from utils.logger import get_logger
from tokenizers import Tokenizer


class InverseSquareRootLRScheduler:
    &#34;&#34;&#34;
    Implements a learning rate scheduler with inverse square root decay.
    
    ...
    
    Attributes:
        optimizer (torch.optim): The optimizer to adjust the learning rate for.
        init_lr (float): The initial learning rate.
        max_lr (float): The maximum learning rate.
        n_warmup_steps (int): The number of warmup steps.
        lr_step (float): The learning rate step.
        decay_factor (float): The decay factor.
        n_steps (int): The number of steps.
    &#34;&#34;&#34;

    def __init__(
        self, optimizer: torch.optim, init_lr: float, max_lr: float, n_warmup_steps: int
    ):
        &#34;&#34;&#34;
        Initialize the scheduler.

        Args:
            optimizer: The optimizer to adjust the learning rate for.
            init_lr (float): The initial learning rate.
            max_lr (float): The maximum learning rate.
            n_warmup_steps (int): The number of warmup steps.
        &#34;&#34;&#34;

        self.optimizer = optimizer
        self.init_lr = init_lr
        self.max_lr = max_lr
        self.n_warmup_steps = n_warmup_steps
        self.lr_step = (max_lr - init_lr) / n_warmup_steps
        self.decay_factor = max_lr * n_warmup_steps**0.5
        self.n_steps = 0

    def step(self):
        &#34;&#34;&#34;
        Update the learning rate for the optimizer.
        &#34;&#34;&#34;

        self.n_steps += 1

        if self.n_steps &lt; self.n_warmup_steps:
            self.lr = self.init_lr + self.n_steps * self.lr_step
        else:
            self.lr = self.decay_factor * self.n_steps**-0.5

        for param_group in self.optimizer.param_groups:
            param_group[&#34;lr&#34;] = self.lr

    def get_lr(self) -&gt; float:
        &#34;&#34;&#34;
        Get the current learning rate.
        &#34;&#34;&#34;

        return self.optimizer.param_groups[0][&#34;lr&#34;]


class LinearWarmupDecayLRScheduler:
    &#34;&#34;&#34;
    Implements a learning rate scheduler with linear warmup and decay.
    
    ...
    
    Attributes:
        optimizer (torch.optim): The optimizer to adjust the learning rate for.
        init_lr (float): The initial learning rate.
        max_lr (float): The maximum learning rate.
        n_warmup_steps (int): The number of warmup steps.
        total_steps (int): The total number of steps.
        lr_step (float): The learning rate step.
        decay_factor (float): The decay factor.
    &#34;&#34;&#34;

    def __init__(
        self,
        optimizer: torch.optim,
        init_lr,
        max_lr: float,
        n_warmup_steps: int,
        total_steps: int,
    ):
        &#34;&#34;&#34;
        Initialize the scheduler.

        Args:
            optimizer (torch.optim): The optimizer to adjust the learning rate for.
            init_lr (float): The initial learning rate.
            max_lr (float): The maximum learning rate.
            total_steps (int): The total number of steps.
            n_warmup_steps (int): The number of warmup steps.
            warmup_lr_step (float): The learning rate step for the warmup phase.
            n_decay_steps (int): The number of decay steps.
            n_steps (int): The number of steps.
        &#34;&#34;&#34;

        self.optimizer = optimizer
        self.init_lr = init_lr
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.n_warmup_steps = n_warmup_steps
        self.warmup_lr_step = (max_lr - init_lr) / n_warmup_steps
        self.n_decay_steps = total_steps - n_warmup_steps
        self.n_steps = 0

    def step(self):
        &#34;&#34;&#34;
        Update the learning rate for the optimizer.
        &#34;&#34;&#34;

        self.n_steps += 1

        if self.n_steps &lt; self.n_warmup_steps:
            self.lr = self.init_lr + self.n_steps * self.warmup_lr_step
        else:
            self.lr = (
                self.max_lr
                / self.n_decay_steps
                * (self.n_decay_steps - (self.n_steps - self.n_warmup_steps))
            )

        for param_group in self.optimizer.param_groups:
            param_group[&#34;lr&#34;] = self.lr

    def get_lr(self) -&gt; float:
        &#34;&#34;&#34;
        Get the current learning rate.
        &#34;&#34;&#34;

        return self.optimizer.param_groups[0][&#34;lr&#34;]


class EarlyStopper:
    &#34;&#34;&#34;
    Implements early stopping to prevent overfitting during training.
    
    ...
    
    Attributes:
        warmup (int): The number of warmup epochs.
        patience (int): The number of epochs to wait before stopping.
        min_delta (int): The minimum change in validation loss to qualify as an improvement.
        counter (int): The number of epochs without improvement.
        min_validation_loss (float): The minimum validation loss.
        logger (logging.Logger): The logger.
    &#34;&#34;&#34;

    def __init__(self, warmup: int = 5, patience: int = 1, min_delta: int = 0):
        &#34;&#34;&#34;
        Initialize the early stopper.

        Args:
            warmup: The number of warmup epochs. Defaults to 5.
            patience: The number of epochs to wait before stopping. Defaults to 1.
            min_delta: The minimum change in validation loss to qualify as an improvement. Defaults to 0.
        &#34;&#34;&#34;

        self.warmup = warmup
        self.patience: int = patience
        self.min_delta: int = min_delta
        self.counter: int = 0
        self.min_validation_loss: float = float(&#34;inf&#34;)
        self.logger = get_logger(&#34;EarlyStopper&#34;)

    def early_stop(self, epoch: int, validation_loss: float) -&gt; bool:
        &#34;&#34;&#34;
        Check if early stopping criterion is met.

        Args:
            epoch (int): The current epoch.
            validation_loss (float): The validation loss.

        Returns:
            bool: True if early stopping criterion is met, False otherwise.
        &#34;&#34;&#34;

        if epoch &lt; self.warmup:
            return False
        if validation_loss &lt; self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss &gt; (self.min_validation_loss + self.min_delta):
            self.counter += 1
            self.logger.info(
                f&#34;{self.counter} epochs without improvement. {self.patience - self.counter} epochs left unless model improves.&#34;
            )
            if self.counter &gt;= self.patience:
                return True
        return False


class Trainer:
    &#34;&#34;&#34;
    Implements a trainer for a sequence-to-sequence model.
    
    ...
    
    Attributes:
        logger (logging.Logger): The logger.
        device (torch.device): The device to use.
        train_loss_values (list): The list of training loss values.
        test_loss_values (list): The list of test loss values.
        learning_rate_values (list): The list of learning rate values.
        test_loss_steps (list): The list of test loss steps.
        use_amp (bool): Whether to use automatic mixed precision.
        scaler (torch.cuda.amp.GradScaler): The gradient scaler.
        dataloaders (dict): The dataloaders.
    &#34;&#34;&#34;
    def __init__(self, device: torch.device):
        &#34;&#34;&#34;
        Initialize the trainer.
        
        ...
        
        Args:
            device (torch.device): The device to use.
        &#34;&#34;&#34;
        self.logger = get_logger(&#34;Trainer&#34;)

        self.device = device

        self.train_loss_values = []
        self.test_loss_values = []
        self.learning_rate_values = []
        self.test_loss_steps = []

        self.use_amp = True
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)
        self.dataloaders = {}

    @classmethod
    def new_instance(
        cls,
        model: Seq2SeqTransformer,
        translator: Processor,
        train_dataloader,
        test_dataloader,
        val_dataloader,
        tokenizer: Tokenizer,
        early_stopper: EarlyStopper,
        trainer_config: TrainerConfig,
        device: torch.device,
        run_id: str,
    ):
        &#34;&#34;&#34;
        Creates a new instance of the Trainer class with the provided configuration.

        Args:
            model (Seq2SeqTransformer): The sequence-to-sequence transformer model.
            translator (src.Processor): The translator object used for the model.
            train_dataloader: The training data loader.
            test_dataloader: The test data loader.
            val_dataloader: The validation data loader.
            tokenizer (tokenizers.Tokenizer): The tokenizer used for the model.
            early_stopper (EarlyStopper): The early stopping object.
            trainer_config (TrainerConfig): The configuration for the trainer.
            device (torch.device): The device to use for training.
            run_id (str): The ID of the current training run.

        Returns:
            Trainer: A new instance of the Trainer class with the provided configuration.
        &#34;&#34;&#34;
        trainer = cls(device)

        trainer.model = model.to(device)
        trainer.translator = translator

        trainer.num_epochs = trainer_config.num_epochs

        trainer.dataloaders[&#34;train&#34;] = train_dataloader
        trainer.dataloaders[&#34;test&#34;] = test_dataloader
        trainer.dataloaders[&#34;val&#34;] = val_dataloader

        trainer.current_epoch = 1
        trainer.step_size = int(
            len(list(trainer.dataloaders[&#34;train&#34;]))
            / (trainer_config.tgt_batch_size / trainer_config.batch_size)
        )
        trainer.tokenizer = tokenizer
        trainer.early_stopper = early_stopper

        trainer.run_id = run_id

        trainer.criterion = nn.CrossEntropyLoss(ignore_index=3)
        trainer.optim = torch.optim.Adam(
            trainer.model.parameters(),
            lr=trainer_config.learning_rate,
            betas=(0.9, 0.98),
            eps=10e-9,
        )

        init_lr = 2e-6

        if trainer_config.lr_scheduler == &#34;inverse_square_root&#34;:
            trainer.scheduler = InverseSquareRootLRScheduler(
                optimizer=trainer.optim,
                init_lr=2e-6,
                max_lr=trainer_config.learning_rate,
                n_warmup_steps=trainer_config.warmup_steps,
            )
        elif trainer_config.lr_scheduler == &#34;linear&#34;:
            total_steps = int(
                trainer.num_epochs
                * len(list(trainer.dataloaders[&#34;train&#34;]))
                / (trainer_config.tgt_batch_size / trainer_config.batch_size)
            )
            init_lr = 2e-6
            trainer.scheduler = LinearWarmupDecayLRScheduler(
                trainer.optim,
                init_lr=init_lr,
                max_lr=trainer_config.learning_rate,
                n_warmup_steps=trainer_config.warmup_steps,
                total_steps=total_steps,
            )

        trainer.learning_rate_values.append(init_lr)
        trainer.grad_accum = trainer_config.tgt_batch_size &gt; trainer_config.batch_size

        if trainer.grad_accum:
            trainer.accumulation_steps = (
                trainer_config.tgt_batch_size // trainer.dataloaders[&#34;train&#34;].batch_size
            )
        else:
            trainer.accumulation_steps = 1

        return trainer

    @classmethod
    def evaluate_checkpoint(
        cls,
        checkpoint_path: str,
        tokenizer_path: str,
        val_dataloader: str,
        translator: Processor,
        device: torch.device,
    ):
        &#34;&#34;&#34;
        Evaluates a trained model checkpoint on the validation dataset.

        Args:
            checkpoint_path (str): The path to the saved model checkpoint.
            tokenizer_path (str): The path to the saved tokenizer.
            val_dataloader (str): The validation data loader.
            translator (Processor): The translator object used for the model.
            device (torch.device): The device to use for evaluation.

        Returns:
            Tuple[float, float]: The BLEU and ROUGE scores for the evaluated model.
        &#34;&#34;&#34;
        trainer = cls(device)

        trainer.model = torch.jit.load(checkpoint_path, map_location=device)
        trainer.tokenizer = tokenizers.Tokenizer.from_file(tokenizer_path)
        trainer.translator = translator

        trainer.dataloaders[&#34;val&#34;] = val_dataloader

        bleu, rouge = trainer.evaluate(inference=True)

        return bleu, rouge

    @classmethod
    def continue_training(cls, *args, **kwargs):
        return NotImplementedError

    def _train_epoch(self) -&gt; float:
        &#34;&#34;&#34;
        Train the model for one epoch.

        Returns:
            float: The average training loss for the epoch.
        &#34;&#34;&#34;

        self.model.train()
        losses = 0
        for batch_idx, (src, tgt) in enumerate(self.dataloaders[&#34;train&#34;]):
            tgt = tgt.type(torch.LongTensor)
            src = src.to(self.device)
            tgt = tgt.to(self.device)

            tgt_input = tgt[:-1, :]
            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                self.translator.create_mask(src, tgt_input)
            )

            with torch.autocast(
                device_type=self.device, dtype=torch.float16, enabled=self.use_amp
            ):
                logits = self.model(
                    src,
                    tgt_input,
                    src_mask,
                    tgt_mask,
                    src_padding_mask,
                    tgt_padding_mask,
                )
                tgt_out = tgt[1:, :]
                loss = self.criterion(
                    logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)
                )

            self.scaler.scale(loss).backward()

            if self.grad_accum:
                for param in self.model.parameters():
                    param.grad /= self.accumulation_steps

            if (batch_idx + 1) % self.accumulation_steps == 0:
                self.scaler.step(self.optim)
                self.scaler.update()
                self.scheduler.step()
                self.learning_rate_values.append(self.scheduler.get_lr())
                # Reset gradients, for the next accumulated batches
                for param in self.model.parameters():
                    param.grad = None
                self.train_loss_values.append(loss.item())

            losses += loss.item()

        return losses / len(list(self.dataloaders[&#34;train&#34;]))

    def _test_epoch(self) -&gt; float:
        &#34;&#34;&#34;
        Test the model for one epoch.

        Returns:
            float: The average test loss for the epoch.
        &#34;&#34;&#34;

        self.model.eval()
        losses = 0
        with torch.no_grad():
            for src, tgt in self.dataloaders[&#34;test&#34;]:
                tgt = tgt.type(torch.LongTensor)
                src = src.to(self.device)
                tgt = tgt.to(self.device)

                tgt_input = tgt[:-1, :]
                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                    self.translator.create_mask(src, tgt_input)
                )

                with torch.autocast(
                    device_type=self.device, dtype=torch.float16, enabled=self.use_amp
                ):
                    logits = self.model(
                        src,
                        tgt_input,
                        src_mask,
                        tgt_mask,
                        src_padding_mask,
                        tgt_padding_mask,
                    )
                    tgt_out = tgt[1:, :]
                    loss = self.criterion(
                        logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)
                    )

                losses += loss.item()

        return losses / len(list(self.dataloaders[&#34;test&#34;]))

    def evaluate(self, inference: bool = False) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;
        Evaluate the model on the validation set and compute the average BLEU and ROUGE scores.

        Args:
            inference (bool, optional): If True, evaluate the model in inference mode without using autocast. Defaults to False.

        Returns:
            Tuple[float, float]: The average BLEU and ROUGE scores on the validation set.
        &#34;&#34;&#34;
        self.model.eval()
        avg_bleu = 0
        avg_rouge = 0
        bleu = load_metric(&#34;bleu&#34;)
        rouge = load_metric(&#34;rouge&#34;)

        with torch.no_grad():
            for batch_idx, (src, tgt) in enumerate(self.dataloaders[&#34;val&#34;]):
                self.logger.info(
                    f&#39;Evaluating batch {batch_idx+1}/{len(list(self.dataloaders[&#34;val&#34;]))}&#39;
                )
                tgt = tgt.type(torch.LongTensor)
                src = src.to(self.device)
                tgt = tgt.to(self.device)

                tgt_input = tgt[:-1, :]
                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                    self.translator.create_mask(src, tgt_input)
                )

                if inference:
                    with torch.no_grad():
                        logits = self.model(
                            src,
                            tgt_input,
                            src_mask,
                            tgt_mask,
                            src_padding_mask,
                            tgt_padding_mask,
                        )
                else:
                    with torch.autocast(
                        device_type=self.device,
                        dtype=torch.float16,
                        enabled=self.use_amp,
                    ):
                        logits = self.model(
                            src,
                            tgt_input,
                            src_mask,
                            tgt_mask,
                            src_padding_mask,
                            tgt_padding_mask,
                        )

                predictions = torch.argmax(logits, dim=-1)
                predictions = predictions.T.cpu().numpy().tolist()
                targets = tgt_input.T.cpu().numpy().tolist()

                all_preds = self.tokenizer.decode_batch(predictions)
                all_targets = self.tokenizer.decode_batch(targets)

                bleu_score = bleu.compute(predictions=all_preds, references=all_targets)
                avg_bleu += bleu_score[&#34;bleu&#34;]

                rouge_score = rouge.compute(
                    predictions=all_preds, references=all_targets
                )
                avg_rouge += rouge_score[&#34;rougeLsum&#34;]

        avg_bleu /= len(list(self.dataloaders[&#34;val&#34;]))
        avg_rouge /= len(list(self.dataloaders[&#34;val&#34;]))

        return avg_bleu, avg_rouge

    def train(self):
        &#34;&#34;&#34;
        Train the model until convergence or early stopping.
        &#34;&#34;&#34;
        try:
            for epoch in range(self.current_epoch, self.num_epochs + 1):
                start_time = perf_counter()
                train_loss = self._train_epoch()
                self.logger.info(
                    f&#34;epoch {epoch} avg_training_loss: {round(train_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
                )

                start_time = perf_counter()
                test_loss = self._test_epoch()
                self.logger.info(
                    f&#34;epoch {epoch} avg_test_loss: {round(test_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
                )

                self.test_loss_values.append(test_loss)
                self.test_loss_steps.append(self.current_epoch * self.step_size)

                self.current_epoch += 1

                self._plot()

                early_stop_true = self.early_stopper.early_stop(epoch, test_loss)
                counter = self.early_stopper.counter

                if counter == 1:
                    self._save_model(&#34;best_&#34;)

                if early_stop_true:
                    self._save_model(&#34;last_&#34;)
                    break
        except KeyboardInterrupt:
            self.logger.error(&#34;Training interrupted by user&#34;)
            self._save_model()
            sys.exit(0)

        self._save_model()

    def _save_model(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint.

        Args:
            name (str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        self._save_model_infer(name)
        self._save_model_train(name)

    def _save_model_infer(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint for inference.

        Args:
            name (str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        model_filepath = f&#34;./models/{self.run_id}/{name}checkpoint_scripted.pt&#34;

        model_scripted = torch.jit.script(self.model)
        model_scripted.save(model_filepath)
        self.logger.info(f&#34;Saved model checkpoint to {model_filepath}&#34;)

    def _save_model_train(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint for further training.

        Args:
            name(str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        model_filepath = f&#34;./models/{self.run_id}/{name}checkpoint.pt&#34;

        torch.save(
            {
                &#34;epoch&#34;: self.current_epoch,
                &#34;model_state_dict&#34;: self.model.state_dict(),
                &#34;optimizer_state_dict&#34;: self.optim.state_dict(),
                # &#39;scheduler_state_dict&#39;: self.scheduler.state_dict(),
            },
            model_filepath,
        )

        self.logger.info(f&#34;Saved model checkpoint to {model_filepath}&#34;)

    def load_model(self):
        &#34;&#34;&#34;
        Load the model from a checkpoint.
        &#34;&#34;&#34;

        filepath = f&#34;./models/{self.run_id}/checkpoint.pt&#34;
        self.model = torch.jit.script(filepath)
        self.logger.info(f&#34;Model checkpoint have been loaded from {filepath}&#34;)

    def _plot(self):
        &#34;&#34;&#34;
        Plot the learning rate, training loss, and test loss metrics during training.
        &#34;&#34;&#34;
        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.learning_rate_values, label=&#34;Learning Rate&#34;)
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Learning Rate&#34;)
        plt.title(&#34;Learning Rate Scheduler&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/learning_rate.png&#34;)
        plt.close()  # Clear the current figure

        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.train_loss_values, label=&#34;Acutal Train Loss&#34;)
        plt.plot(
            self._smooth(scalars=self.train_loss_values, weight=0.9),
            label=&#34;Smoothed Train Loss&#34;,
        )
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Loss&#34;)
        plt.title(&#34;Training loss vs. steps&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/train_loss.png&#34;)
        plt.close()  # Clear the current figure

        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.test_loss_steps, self.test_loss_values, label=&#34;Test Loss&#34;)
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Loss&#34;)
        plt.title(&#34;Test loss vs. steps&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/test_loss.png&#34;)
        plt.close()  # Clear the current figure

    @staticmethod
    def _smooth(
        scalars: List[float], weight: float
    ) -&gt; List[float]:  # Weight between 0 and 1
        last = scalars[0]  # First value in the plot (first timestep)
        smoothed = list()
        for point in scalars:
            smoothed_val = (
                last * weight + (1 - weight) * point
            )  # Calculate smoothed value
            smoothed.append(smoothed_val)  # Save it
            last = smoothed_val  # Anchor the last smoothed value

        return smoothed</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="transformer.src.trainer.EarlyStopper"><code class="flex name class">
<span>class <span class="ident">EarlyStopper</span></span>
<span>(</span><span>warmup: int = 5, patience: int = 1, min_delta: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements early stopping to prevent overfitting during training.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>warmup</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of warmup epochs.</dd>
<dt><strong><code>patience</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to wait before stopping.</dd>
<dt><strong><code>min_delta</code></strong> :&ensp;<code>int</code></dt>
<dd>The minimum change in validation loss to qualify as an improvement.</dd>
<dt><strong><code>counter</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs without improvement.</dd>
<dt><strong><code>min_validation_loss</code></strong> :&ensp;<code>float</code></dt>
<dd>The minimum validation loss.</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>logging.Logger</code></dt>
<dd>The logger.</dd>
</dl>
<p>Initialize the early stopper.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>warmup</code></strong></dt>
<dd>The number of warmup epochs. Defaults to 5.</dd>
<dt><strong><code>patience</code></strong></dt>
<dd>The number of epochs to wait before stopping. Defaults to 1.</dd>
<dt><strong><code>min_delta</code></strong></dt>
<dd>The minimum change in validation loss to qualify as an improvement. Defaults to 0.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EarlyStopper:
    &#34;&#34;&#34;
    Implements early stopping to prevent overfitting during training.
    
    ...
    
    Attributes:
        warmup (int): The number of warmup epochs.
        patience (int): The number of epochs to wait before stopping.
        min_delta (int): The minimum change in validation loss to qualify as an improvement.
        counter (int): The number of epochs without improvement.
        min_validation_loss (float): The minimum validation loss.
        logger (logging.Logger): The logger.
    &#34;&#34;&#34;

    def __init__(self, warmup: int = 5, patience: int = 1, min_delta: int = 0):
        &#34;&#34;&#34;
        Initialize the early stopper.

        Args:
            warmup: The number of warmup epochs. Defaults to 5.
            patience: The number of epochs to wait before stopping. Defaults to 1.
            min_delta: The minimum change in validation loss to qualify as an improvement. Defaults to 0.
        &#34;&#34;&#34;

        self.warmup = warmup
        self.patience: int = patience
        self.min_delta: int = min_delta
        self.counter: int = 0
        self.min_validation_loss: float = float(&#34;inf&#34;)
        self.logger = get_logger(&#34;EarlyStopper&#34;)

    def early_stop(self, epoch: int, validation_loss: float) -&gt; bool:
        &#34;&#34;&#34;
        Check if early stopping criterion is met.

        Args:
            epoch (int): The current epoch.
            validation_loss (float): The validation loss.

        Returns:
            bool: True if early stopping criterion is met, False otherwise.
        &#34;&#34;&#34;

        if epoch &lt; self.warmup:
            return False
        if validation_loss &lt; self.min_validation_loss:
            self.min_validation_loss = validation_loss
            self.counter = 0
        elif validation_loss &gt; (self.min_validation_loss + self.min_delta):
            self.counter += 1
            self.logger.info(
                f&#34;{self.counter} epochs without improvement. {self.patience - self.counter} epochs left unless model improves.&#34;
            )
            if self.counter &gt;= self.patience:
                return True
        return False</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.trainer.EarlyStopper.early_stop"><code class="name flex">
<span>def <span class="ident">early_stop</span></span>(<span>self, epoch: int, validation_loss: float) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if early stopping criterion is met.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>The current epoch.</dd>
<dt><strong><code>validation_loss</code></strong> :&ensp;<code>float</code></dt>
<dd>The validation loss.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>True if early stopping criterion is met, False otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def early_stop(self, epoch: int, validation_loss: float) -&gt; bool:
    &#34;&#34;&#34;
    Check if early stopping criterion is met.

    Args:
        epoch (int): The current epoch.
        validation_loss (float): The validation loss.

    Returns:
        bool: True if early stopping criterion is met, False otherwise.
    &#34;&#34;&#34;

    if epoch &lt; self.warmup:
        return False
    if validation_loss &lt; self.min_validation_loss:
        self.min_validation_loss = validation_loss
        self.counter = 0
    elif validation_loss &gt; (self.min_validation_loss + self.min_delta):
        self.counter += 1
        self.logger.info(
            f&#34;{self.counter} epochs without improvement. {self.patience - self.counter} epochs left unless model improves.&#34;
        )
        if self.counter &gt;= self.patience:
            return True
    return False</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="transformer.src.trainer.InverseSquareRootLRScheduler"><code class="flex name class">
<span>class <span class="ident">InverseSquareRootLRScheduler</span></span>
<span>(</span><span>optimizer: <module 'torch.optim' from '/Users/nicofuchs/Coding/python/<a title="transformer" href="../index.html">transformer</a>/.venv/lib/python3.10/site-packages/torch/optim/__init__.py'>, init_lr: float, max_lr: float, n_warmup_steps: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements a learning rate scheduler with inverse square root decay.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim</code></dt>
<dd>The optimizer to adjust the learning rate for.</dd>
<dt><strong><code>init_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The initial learning rate.</dd>
<dt><strong><code>max_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum learning rate.</dd>
<dt><strong><code>n_warmup_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of warmup steps.</dd>
<dt><strong><code>lr_step</code></strong> :&ensp;<code>float</code></dt>
<dd>The learning rate step.</dd>
<dt><strong><code>decay_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>The decay factor.</dd>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of steps.</dd>
</dl>
<p>Initialize the scheduler.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>optimizer</code></strong></dt>
<dd>The optimizer to adjust the learning rate for.</dd>
<dt><strong><code>init_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The initial learning rate.</dd>
<dt><strong><code>max_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum learning rate.</dd>
<dt><strong><code>n_warmup_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of warmup steps.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class InverseSquareRootLRScheduler:
    &#34;&#34;&#34;
    Implements a learning rate scheduler with inverse square root decay.
    
    ...
    
    Attributes:
        optimizer (torch.optim): The optimizer to adjust the learning rate for.
        init_lr (float): The initial learning rate.
        max_lr (float): The maximum learning rate.
        n_warmup_steps (int): The number of warmup steps.
        lr_step (float): The learning rate step.
        decay_factor (float): The decay factor.
        n_steps (int): The number of steps.
    &#34;&#34;&#34;

    def __init__(
        self, optimizer: torch.optim, init_lr: float, max_lr: float, n_warmup_steps: int
    ):
        &#34;&#34;&#34;
        Initialize the scheduler.

        Args:
            optimizer: The optimizer to adjust the learning rate for.
            init_lr (float): The initial learning rate.
            max_lr (float): The maximum learning rate.
            n_warmup_steps (int): The number of warmup steps.
        &#34;&#34;&#34;

        self.optimizer = optimizer
        self.init_lr = init_lr
        self.max_lr = max_lr
        self.n_warmup_steps = n_warmup_steps
        self.lr_step = (max_lr - init_lr) / n_warmup_steps
        self.decay_factor = max_lr * n_warmup_steps**0.5
        self.n_steps = 0

    def step(self):
        &#34;&#34;&#34;
        Update the learning rate for the optimizer.
        &#34;&#34;&#34;

        self.n_steps += 1

        if self.n_steps &lt; self.n_warmup_steps:
            self.lr = self.init_lr + self.n_steps * self.lr_step
        else:
            self.lr = self.decay_factor * self.n_steps**-0.5

        for param_group in self.optimizer.param_groups:
            param_group[&#34;lr&#34;] = self.lr

    def get_lr(self) -&gt; float:
        &#34;&#34;&#34;
        Get the current learning rate.
        &#34;&#34;&#34;

        return self.optimizer.param_groups[0][&#34;lr&#34;]</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.trainer.InverseSquareRootLRScheduler.get_lr"><code class="name flex">
<span>def <span class="ident">get_lr</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Get the current learning rate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lr(self) -&gt; float:
    &#34;&#34;&#34;
    Get the current learning rate.
    &#34;&#34;&#34;

    return self.optimizer.param_groups[0][&#34;lr&#34;]</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.InverseSquareRootLRScheduler.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the learning rate for the optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self):
    &#34;&#34;&#34;
    Update the learning rate for the optimizer.
    &#34;&#34;&#34;

    self.n_steps += 1

    if self.n_steps &lt; self.n_warmup_steps:
        self.lr = self.init_lr + self.n_steps * self.lr_step
    else:
        self.lr = self.decay_factor * self.n_steps**-0.5

    for param_group in self.optimizer.param_groups:
        param_group[&#34;lr&#34;] = self.lr</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="transformer.src.trainer.LinearWarmupDecayLRScheduler"><code class="flex name class">
<span>class <span class="ident">LinearWarmupDecayLRScheduler</span></span>
<span>(</span><span>optimizer: <module 'torch.optim' from '/Users/nicofuchs/Coding/python/<a title="transformer" href="../index.html">transformer</a>/.venv/lib/python3.10/site-packages/torch/optim/__init__.py'>, init_lr, max_lr: float, n_warmup_steps: int, total_steps: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements a learning rate scheduler with linear warmup and decay.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim</code></dt>
<dd>The optimizer to adjust the learning rate for.</dd>
<dt><strong><code>init_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The initial learning rate.</dd>
<dt><strong><code>max_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum learning rate.</dd>
<dt><strong><code>n_warmup_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of warmup steps.</dd>
<dt><strong><code>total_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The total number of steps.</dd>
<dt><strong><code>lr_step</code></strong> :&ensp;<code>float</code></dt>
<dd>The learning rate step.</dd>
<dt><strong><code>decay_factor</code></strong> :&ensp;<code>float</code></dt>
<dd>The decay factor.</dd>
</dl>
<p>Initialize the scheduler.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim</code></dt>
<dd>The optimizer to adjust the learning rate for.</dd>
<dt><strong><code>init_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The initial learning rate.</dd>
<dt><strong><code>max_lr</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum learning rate.</dd>
<dt><strong><code>total_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The total number of steps.</dd>
<dt><strong><code>n_warmup_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of warmup steps.</dd>
<dt><strong><code>warmup_lr_step</code></strong> :&ensp;<code>float</code></dt>
<dd>The learning rate step for the warmup phase.</dd>
<dt><strong><code>n_decay_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of decay steps.</dd>
<dt><strong><code>n_steps</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of steps.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LinearWarmupDecayLRScheduler:
    &#34;&#34;&#34;
    Implements a learning rate scheduler with linear warmup and decay.
    
    ...
    
    Attributes:
        optimizer (torch.optim): The optimizer to adjust the learning rate for.
        init_lr (float): The initial learning rate.
        max_lr (float): The maximum learning rate.
        n_warmup_steps (int): The number of warmup steps.
        total_steps (int): The total number of steps.
        lr_step (float): The learning rate step.
        decay_factor (float): The decay factor.
    &#34;&#34;&#34;

    def __init__(
        self,
        optimizer: torch.optim,
        init_lr,
        max_lr: float,
        n_warmup_steps: int,
        total_steps: int,
    ):
        &#34;&#34;&#34;
        Initialize the scheduler.

        Args:
            optimizer (torch.optim): The optimizer to adjust the learning rate for.
            init_lr (float): The initial learning rate.
            max_lr (float): The maximum learning rate.
            total_steps (int): The total number of steps.
            n_warmup_steps (int): The number of warmup steps.
            warmup_lr_step (float): The learning rate step for the warmup phase.
            n_decay_steps (int): The number of decay steps.
            n_steps (int): The number of steps.
        &#34;&#34;&#34;

        self.optimizer = optimizer
        self.init_lr = init_lr
        self.max_lr = max_lr
        self.total_steps = total_steps
        self.n_warmup_steps = n_warmup_steps
        self.warmup_lr_step = (max_lr - init_lr) / n_warmup_steps
        self.n_decay_steps = total_steps - n_warmup_steps
        self.n_steps = 0

    def step(self):
        &#34;&#34;&#34;
        Update the learning rate for the optimizer.
        &#34;&#34;&#34;

        self.n_steps += 1

        if self.n_steps &lt; self.n_warmup_steps:
            self.lr = self.init_lr + self.n_steps * self.warmup_lr_step
        else:
            self.lr = (
                self.max_lr
                / self.n_decay_steps
                * (self.n_decay_steps - (self.n_steps - self.n_warmup_steps))
            )

        for param_group in self.optimizer.param_groups:
            param_group[&#34;lr&#34;] = self.lr

    def get_lr(self) -&gt; float:
        &#34;&#34;&#34;
        Get the current learning rate.
        &#34;&#34;&#34;

        return self.optimizer.param_groups[0][&#34;lr&#34;]</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.trainer.LinearWarmupDecayLRScheduler.get_lr"><code class="name flex">
<span>def <span class="ident">get_lr</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>Get the current learning rate.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lr(self) -&gt; float:
    &#34;&#34;&#34;
    Get the current learning rate.
    &#34;&#34;&#34;

    return self.optimizer.param_groups[0][&#34;lr&#34;]</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.LinearWarmupDecayLRScheduler.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the learning rate for the optimizer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self):
    &#34;&#34;&#34;
    Update the learning rate for the optimizer.
    &#34;&#34;&#34;

    self.n_steps += 1

    if self.n_steps &lt; self.n_warmup_steps:
        self.lr = self.init_lr + self.n_steps * self.warmup_lr_step
    else:
        self.lr = (
            self.max_lr
            / self.n_decay_steps
            * (self.n_decay_steps - (self.n_steps - self.n_warmup_steps))
        )

    for param_group in self.optimizer.param_groups:
        param_group[&#34;lr&#34;] = self.lr</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="transformer.src.trainer.Trainer"><code class="flex name class">
<span>class <span class="ident">Trainer</span></span>
<span>(</span><span>device: torch.device)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements a trainer for a sequence-to-sequence model.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>logger</code></strong> :&ensp;<code>logging.Logger</code></dt>
<dd>The logger.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device to use.</dd>
<dt><strong><code>train_loss_values</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of training loss values.</dd>
<dt><strong><code>test_loss_values</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of test loss values.</dd>
<dt><strong><code>learning_rate_values</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of learning rate values.</dd>
<dt><strong><code>test_loss_steps</code></strong> :&ensp;<code>list</code></dt>
<dd>The list of test loss steps.</dd>
<dt><strong><code>use_amp</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use automatic mixed precision.</dd>
<dt><strong><code>scaler</code></strong> :&ensp;<code>torch.cuda.amp.GradScaler</code></dt>
<dd>The gradient scaler.</dd>
<dt><strong><code>dataloaders</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dataloaders.</dd>
</dl>
<p>Initialize the trainer.</p>
<p>&hellip;</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device to use.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Trainer:
    &#34;&#34;&#34;
    Implements a trainer for a sequence-to-sequence model.
    
    ...
    
    Attributes:
        logger (logging.Logger): The logger.
        device (torch.device): The device to use.
        train_loss_values (list): The list of training loss values.
        test_loss_values (list): The list of test loss values.
        learning_rate_values (list): The list of learning rate values.
        test_loss_steps (list): The list of test loss steps.
        use_amp (bool): Whether to use automatic mixed precision.
        scaler (torch.cuda.amp.GradScaler): The gradient scaler.
        dataloaders (dict): The dataloaders.
    &#34;&#34;&#34;
    def __init__(self, device: torch.device):
        &#34;&#34;&#34;
        Initialize the trainer.
        
        ...
        
        Args:
            device (torch.device): The device to use.
        &#34;&#34;&#34;
        self.logger = get_logger(&#34;Trainer&#34;)

        self.device = device

        self.train_loss_values = []
        self.test_loss_values = []
        self.learning_rate_values = []
        self.test_loss_steps = []

        self.use_amp = True
        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)
        self.dataloaders = {}

    @classmethod
    def new_instance(
        cls,
        model: Seq2SeqTransformer,
        translator: Processor,
        train_dataloader,
        test_dataloader,
        val_dataloader,
        tokenizer: Tokenizer,
        early_stopper: EarlyStopper,
        trainer_config: TrainerConfig,
        device: torch.device,
        run_id: str,
    ):
        &#34;&#34;&#34;
        Creates a new instance of the Trainer class with the provided configuration.

        Args:
            model (Seq2SeqTransformer): The sequence-to-sequence transformer model.
            translator (src.Processor): The translator object used for the model.
            train_dataloader: The training data loader.
            test_dataloader: The test data loader.
            val_dataloader: The validation data loader.
            tokenizer (tokenizers.Tokenizer): The tokenizer used for the model.
            early_stopper (EarlyStopper): The early stopping object.
            trainer_config (TrainerConfig): The configuration for the trainer.
            device (torch.device): The device to use for training.
            run_id (str): The ID of the current training run.

        Returns:
            Trainer: A new instance of the Trainer class with the provided configuration.
        &#34;&#34;&#34;
        trainer = cls(device)

        trainer.model = model.to(device)
        trainer.translator = translator

        trainer.num_epochs = trainer_config.num_epochs

        trainer.dataloaders[&#34;train&#34;] = train_dataloader
        trainer.dataloaders[&#34;test&#34;] = test_dataloader
        trainer.dataloaders[&#34;val&#34;] = val_dataloader

        trainer.current_epoch = 1
        trainer.step_size = int(
            len(list(trainer.dataloaders[&#34;train&#34;]))
            / (trainer_config.tgt_batch_size / trainer_config.batch_size)
        )
        trainer.tokenizer = tokenizer
        trainer.early_stopper = early_stopper

        trainer.run_id = run_id

        trainer.criterion = nn.CrossEntropyLoss(ignore_index=3)
        trainer.optim = torch.optim.Adam(
            trainer.model.parameters(),
            lr=trainer_config.learning_rate,
            betas=(0.9, 0.98),
            eps=10e-9,
        )

        init_lr = 2e-6

        if trainer_config.lr_scheduler == &#34;inverse_square_root&#34;:
            trainer.scheduler = InverseSquareRootLRScheduler(
                optimizer=trainer.optim,
                init_lr=2e-6,
                max_lr=trainer_config.learning_rate,
                n_warmup_steps=trainer_config.warmup_steps,
            )
        elif trainer_config.lr_scheduler == &#34;linear&#34;:
            total_steps = int(
                trainer.num_epochs
                * len(list(trainer.dataloaders[&#34;train&#34;]))
                / (trainer_config.tgt_batch_size / trainer_config.batch_size)
            )
            init_lr = 2e-6
            trainer.scheduler = LinearWarmupDecayLRScheduler(
                trainer.optim,
                init_lr=init_lr,
                max_lr=trainer_config.learning_rate,
                n_warmup_steps=trainer_config.warmup_steps,
                total_steps=total_steps,
            )

        trainer.learning_rate_values.append(init_lr)
        trainer.grad_accum = trainer_config.tgt_batch_size &gt; trainer_config.batch_size

        if trainer.grad_accum:
            trainer.accumulation_steps = (
                trainer_config.tgt_batch_size // trainer.dataloaders[&#34;train&#34;].batch_size
            )
        else:
            trainer.accumulation_steps = 1

        return trainer

    @classmethod
    def evaluate_checkpoint(
        cls,
        checkpoint_path: str,
        tokenizer_path: str,
        val_dataloader: str,
        translator: Processor,
        device: torch.device,
    ):
        &#34;&#34;&#34;
        Evaluates a trained model checkpoint on the validation dataset.

        Args:
            checkpoint_path (str): The path to the saved model checkpoint.
            tokenizer_path (str): The path to the saved tokenizer.
            val_dataloader (str): The validation data loader.
            translator (Processor): The translator object used for the model.
            device (torch.device): The device to use for evaluation.

        Returns:
            Tuple[float, float]: The BLEU and ROUGE scores for the evaluated model.
        &#34;&#34;&#34;
        trainer = cls(device)

        trainer.model = torch.jit.load(checkpoint_path, map_location=device)
        trainer.tokenizer = tokenizers.Tokenizer.from_file(tokenizer_path)
        trainer.translator = translator

        trainer.dataloaders[&#34;val&#34;] = val_dataloader

        bleu, rouge = trainer.evaluate(inference=True)

        return bleu, rouge

    @classmethod
    def continue_training(cls, *args, **kwargs):
        return NotImplementedError

    def _train_epoch(self) -&gt; float:
        &#34;&#34;&#34;
        Train the model for one epoch.

        Returns:
            float: The average training loss for the epoch.
        &#34;&#34;&#34;

        self.model.train()
        losses = 0
        for batch_idx, (src, tgt) in enumerate(self.dataloaders[&#34;train&#34;]):
            tgt = tgt.type(torch.LongTensor)
            src = src.to(self.device)
            tgt = tgt.to(self.device)

            tgt_input = tgt[:-1, :]
            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                self.translator.create_mask(src, tgt_input)
            )

            with torch.autocast(
                device_type=self.device, dtype=torch.float16, enabled=self.use_amp
            ):
                logits = self.model(
                    src,
                    tgt_input,
                    src_mask,
                    tgt_mask,
                    src_padding_mask,
                    tgt_padding_mask,
                )
                tgt_out = tgt[1:, :]
                loss = self.criterion(
                    logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)
                )

            self.scaler.scale(loss).backward()

            if self.grad_accum:
                for param in self.model.parameters():
                    param.grad /= self.accumulation_steps

            if (batch_idx + 1) % self.accumulation_steps == 0:
                self.scaler.step(self.optim)
                self.scaler.update()
                self.scheduler.step()
                self.learning_rate_values.append(self.scheduler.get_lr())
                # Reset gradients, for the next accumulated batches
                for param in self.model.parameters():
                    param.grad = None
                self.train_loss_values.append(loss.item())

            losses += loss.item()

        return losses / len(list(self.dataloaders[&#34;train&#34;]))

    def _test_epoch(self) -&gt; float:
        &#34;&#34;&#34;
        Test the model for one epoch.

        Returns:
            float: The average test loss for the epoch.
        &#34;&#34;&#34;

        self.model.eval()
        losses = 0
        with torch.no_grad():
            for src, tgt in self.dataloaders[&#34;test&#34;]:
                tgt = tgt.type(torch.LongTensor)
                src = src.to(self.device)
                tgt = tgt.to(self.device)

                tgt_input = tgt[:-1, :]
                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                    self.translator.create_mask(src, tgt_input)
                )

                with torch.autocast(
                    device_type=self.device, dtype=torch.float16, enabled=self.use_amp
                ):
                    logits = self.model(
                        src,
                        tgt_input,
                        src_mask,
                        tgt_mask,
                        src_padding_mask,
                        tgt_padding_mask,
                    )
                    tgt_out = tgt[1:, :]
                    loss = self.criterion(
                        logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1)
                    )

                losses += loss.item()

        return losses / len(list(self.dataloaders[&#34;test&#34;]))

    def evaluate(self, inference: bool = False) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;
        Evaluate the model on the validation set and compute the average BLEU and ROUGE scores.

        Args:
            inference (bool, optional): If True, evaluate the model in inference mode without using autocast. Defaults to False.

        Returns:
            Tuple[float, float]: The average BLEU and ROUGE scores on the validation set.
        &#34;&#34;&#34;
        self.model.eval()
        avg_bleu = 0
        avg_rouge = 0
        bleu = load_metric(&#34;bleu&#34;)
        rouge = load_metric(&#34;rouge&#34;)

        with torch.no_grad():
            for batch_idx, (src, tgt) in enumerate(self.dataloaders[&#34;val&#34;]):
                self.logger.info(
                    f&#39;Evaluating batch {batch_idx+1}/{len(list(self.dataloaders[&#34;val&#34;]))}&#39;
                )
                tgt = tgt.type(torch.LongTensor)
                src = src.to(self.device)
                tgt = tgt.to(self.device)

                tgt_input = tgt[:-1, :]
                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                    self.translator.create_mask(src, tgt_input)
                )

                if inference:
                    with torch.no_grad():
                        logits = self.model(
                            src,
                            tgt_input,
                            src_mask,
                            tgt_mask,
                            src_padding_mask,
                            tgt_padding_mask,
                        )
                else:
                    with torch.autocast(
                        device_type=self.device,
                        dtype=torch.float16,
                        enabled=self.use_amp,
                    ):
                        logits = self.model(
                            src,
                            tgt_input,
                            src_mask,
                            tgt_mask,
                            src_padding_mask,
                            tgt_padding_mask,
                        )

                predictions = torch.argmax(logits, dim=-1)
                predictions = predictions.T.cpu().numpy().tolist()
                targets = tgt_input.T.cpu().numpy().tolist()

                all_preds = self.tokenizer.decode_batch(predictions)
                all_targets = self.tokenizer.decode_batch(targets)

                bleu_score = bleu.compute(predictions=all_preds, references=all_targets)
                avg_bleu += bleu_score[&#34;bleu&#34;]

                rouge_score = rouge.compute(
                    predictions=all_preds, references=all_targets
                )
                avg_rouge += rouge_score[&#34;rougeLsum&#34;]

        avg_bleu /= len(list(self.dataloaders[&#34;val&#34;]))
        avg_rouge /= len(list(self.dataloaders[&#34;val&#34;]))

        return avg_bleu, avg_rouge

    def train(self):
        &#34;&#34;&#34;
        Train the model until convergence or early stopping.
        &#34;&#34;&#34;
        try:
            for epoch in range(self.current_epoch, self.num_epochs + 1):
                start_time = perf_counter()
                train_loss = self._train_epoch()
                self.logger.info(
                    f&#34;epoch {epoch} avg_training_loss: {round(train_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
                )

                start_time = perf_counter()
                test_loss = self._test_epoch()
                self.logger.info(
                    f&#34;epoch {epoch} avg_test_loss: {round(test_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
                )

                self.test_loss_values.append(test_loss)
                self.test_loss_steps.append(self.current_epoch * self.step_size)

                self.current_epoch += 1

                self._plot()

                early_stop_true = self.early_stopper.early_stop(epoch, test_loss)
                counter = self.early_stopper.counter

                if counter == 1:
                    self._save_model(&#34;best_&#34;)

                if early_stop_true:
                    self._save_model(&#34;last_&#34;)
                    break
        except KeyboardInterrupt:
            self.logger.error(&#34;Training interrupted by user&#34;)
            self._save_model()
            sys.exit(0)

        self._save_model()

    def _save_model(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint.

        Args:
            name (str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        self._save_model_infer(name)
        self._save_model_train(name)

    def _save_model_infer(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint for inference.

        Args:
            name (str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        model_filepath = f&#34;./models/{self.run_id}/{name}checkpoint_scripted.pt&#34;

        model_scripted = torch.jit.script(self.model)
        model_scripted.save(model_filepath)
        self.logger.info(f&#34;Saved model checkpoint to {model_filepath}&#34;)

    def _save_model_train(self, name=&#34;&#34;):
        &#34;&#34;&#34;
        Save the model checkpoint for further training.

        Args:
            name(str): The name of the model checkpoint. Defaults to &#34;&#34;.
        &#34;&#34;&#34;

        model_filepath = f&#34;./models/{self.run_id}/{name}checkpoint.pt&#34;

        torch.save(
            {
                &#34;epoch&#34;: self.current_epoch,
                &#34;model_state_dict&#34;: self.model.state_dict(),
                &#34;optimizer_state_dict&#34;: self.optim.state_dict(),
                # &#39;scheduler_state_dict&#39;: self.scheduler.state_dict(),
            },
            model_filepath,
        )

        self.logger.info(f&#34;Saved model checkpoint to {model_filepath}&#34;)

    def load_model(self):
        &#34;&#34;&#34;
        Load the model from a checkpoint.
        &#34;&#34;&#34;

        filepath = f&#34;./models/{self.run_id}/checkpoint.pt&#34;
        self.model = torch.jit.script(filepath)
        self.logger.info(f&#34;Model checkpoint have been loaded from {filepath}&#34;)

    def _plot(self):
        &#34;&#34;&#34;
        Plot the learning rate, training loss, and test loss metrics during training.
        &#34;&#34;&#34;
        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.learning_rate_values, label=&#34;Learning Rate&#34;)
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Learning Rate&#34;)
        plt.title(&#34;Learning Rate Scheduler&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/learning_rate.png&#34;)
        plt.close()  # Clear the current figure

        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.train_loss_values, label=&#34;Acutal Train Loss&#34;)
        plt.plot(
            self._smooth(scalars=self.train_loss_values, weight=0.9),
            label=&#34;Smoothed Train Loss&#34;,
        )
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Loss&#34;)
        plt.title(&#34;Training loss vs. steps&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/train_loss.png&#34;)
        plt.close()  # Clear the current figure

        # Plot the learning rate function
        plt.figure(figsize=(8, 6))
        plt.plot(self.test_loss_steps, self.test_loss_values, label=&#34;Test Loss&#34;)
        plt.xlabel(&#34;Step&#34;)
        plt.ylabel(&#34;Loss&#34;)
        plt.title(&#34;Test loss vs. steps&#34;)
        plt.grid(True)

        # Add vertical line at the end of warmup phase
        plt.axvline(
            x=self.scheduler.n_warmup_steps,
            color=&#34;r&#34;,
            linestyle=&#34;--&#34;,
            label=&#34;Warmup End&#34;,
        )

        plt.legend()
        plt.savefig(f&#34;./models/{self.run_id}/metrics/test_loss.png&#34;)
        plt.close()  # Clear the current figure

    @staticmethod
    def _smooth(
        scalars: List[float], weight: float
    ) -&gt; List[float]:  # Weight between 0 and 1
        last = scalars[0]  # First value in the plot (first timestep)
        smoothed = list()
        for point in scalars:
            smoothed_val = (
                last * weight + (1 - weight) * point
            )  # Calculate smoothed value
            smoothed.append(smoothed_val)  # Save it
            last = smoothed_val  # Anchor the last smoothed value

        return smoothed</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="transformer.src.trainer.Trainer.continue_training"><code class="name flex">
<span>def <span class="ident">continue_training</span></span>(<span>*args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def continue_training(cls, *args, **kwargs):
    return NotImplementedError</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.Trainer.evaluate_checkpoint"><code class="name flex">
<span>def <span class="ident">evaluate_checkpoint</span></span>(<span>checkpoint_path: str, tokenizer_path: str, val_dataloader: str, translator: src.processor.Processor, device: torch.device)</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluates a trained model checkpoint on the validation dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>checkpoint_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the saved model checkpoint.</dd>
<dt><strong><code>tokenizer_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the saved tokenizer.</dd>
<dt><strong><code>val_dataloader</code></strong> :&ensp;<code>str</code></dt>
<dd>The validation data loader.</dd>
<dt><strong><code>translator</code></strong> :&ensp;<code>Processor</code></dt>
<dd>The translator object used for the model.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device to use for evaluation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, float]</code></dt>
<dd>The BLEU and ROUGE scores for the evaluated model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def evaluate_checkpoint(
    cls,
    checkpoint_path: str,
    tokenizer_path: str,
    val_dataloader: str,
    translator: Processor,
    device: torch.device,
):
    &#34;&#34;&#34;
    Evaluates a trained model checkpoint on the validation dataset.

    Args:
        checkpoint_path (str): The path to the saved model checkpoint.
        tokenizer_path (str): The path to the saved tokenizer.
        val_dataloader (str): The validation data loader.
        translator (Processor): The translator object used for the model.
        device (torch.device): The device to use for evaluation.

    Returns:
        Tuple[float, float]: The BLEU and ROUGE scores for the evaluated model.
    &#34;&#34;&#34;
    trainer = cls(device)

    trainer.model = torch.jit.load(checkpoint_path, map_location=device)
    trainer.tokenizer = tokenizers.Tokenizer.from_file(tokenizer_path)
    trainer.translator = translator

    trainer.dataloaders[&#34;val&#34;] = val_dataloader

    bleu, rouge = trainer.evaluate(inference=True)

    return bleu, rouge</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.Trainer.new_instance"><code class="name flex">
<span>def <span class="ident">new_instance</span></span>(<span>model: src.seq2seq_transformer.Seq2SeqTransformer, translator: src.processor.Processor, train_dataloader, test_dataloader, val_dataloader, tokenizer: tokenizers.Tokenizer, early_stopper: <a title="transformer.src.trainer.EarlyStopper" href="#transformer.src.trainer.EarlyStopper">EarlyStopper</a>, trainer_config: utils.config.TrainerConfig, device: torch.device, run_id: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new instance of the Trainer class with the provided configuration.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>Seq2SeqTransformer</code></dt>
<dd>The sequence-to-sequence transformer model.</dd>
<dt><strong><code>translator</code></strong> :&ensp;<code>src.Processor</code></dt>
<dd>The translator object used for the model.</dd>
<dt><strong><code>train_dataloader</code></strong></dt>
<dd>The training data loader.</dd>
<dt><strong><code>test_dataloader</code></strong></dt>
<dd>The test data loader.</dd>
<dt><strong><code>val_dataloader</code></strong></dt>
<dd>The validation data loader.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>tokenizers.Tokenizer</code></dt>
<dd>The tokenizer used for the model.</dd>
<dt><strong><code>early_stopper</code></strong> :&ensp;<code><a title="transformer.src.trainer.EarlyStopper" href="#transformer.src.trainer.EarlyStopper">EarlyStopper</a></code></dt>
<dd>The early stopping object.</dd>
<dt><strong><code>trainer_config</code></strong> :&ensp;<code>TrainerConfig</code></dt>
<dd>The configuration for the trainer.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device to use for training.</dd>
<dt><strong><code>run_id</code></strong> :&ensp;<code>str</code></dt>
<dd>The ID of the current training run.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="transformer.src.trainer.Trainer" href="#transformer.src.trainer.Trainer">Trainer</a></code></dt>
<dd>A new instance of the Trainer class with the provided configuration.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def new_instance(
    cls,
    model: Seq2SeqTransformer,
    translator: Processor,
    train_dataloader,
    test_dataloader,
    val_dataloader,
    tokenizer: Tokenizer,
    early_stopper: EarlyStopper,
    trainer_config: TrainerConfig,
    device: torch.device,
    run_id: str,
):
    &#34;&#34;&#34;
    Creates a new instance of the Trainer class with the provided configuration.

    Args:
        model (Seq2SeqTransformer): The sequence-to-sequence transformer model.
        translator (src.Processor): The translator object used for the model.
        train_dataloader: The training data loader.
        test_dataloader: The test data loader.
        val_dataloader: The validation data loader.
        tokenizer (tokenizers.Tokenizer): The tokenizer used for the model.
        early_stopper (EarlyStopper): The early stopping object.
        trainer_config (TrainerConfig): The configuration for the trainer.
        device (torch.device): The device to use for training.
        run_id (str): The ID of the current training run.

    Returns:
        Trainer: A new instance of the Trainer class with the provided configuration.
    &#34;&#34;&#34;
    trainer = cls(device)

    trainer.model = model.to(device)
    trainer.translator = translator

    trainer.num_epochs = trainer_config.num_epochs

    trainer.dataloaders[&#34;train&#34;] = train_dataloader
    trainer.dataloaders[&#34;test&#34;] = test_dataloader
    trainer.dataloaders[&#34;val&#34;] = val_dataloader

    trainer.current_epoch = 1
    trainer.step_size = int(
        len(list(trainer.dataloaders[&#34;train&#34;]))
        / (trainer_config.tgt_batch_size / trainer_config.batch_size)
    )
    trainer.tokenizer = tokenizer
    trainer.early_stopper = early_stopper

    trainer.run_id = run_id

    trainer.criterion = nn.CrossEntropyLoss(ignore_index=3)
    trainer.optim = torch.optim.Adam(
        trainer.model.parameters(),
        lr=trainer_config.learning_rate,
        betas=(0.9, 0.98),
        eps=10e-9,
    )

    init_lr = 2e-6

    if trainer_config.lr_scheduler == &#34;inverse_square_root&#34;:
        trainer.scheduler = InverseSquareRootLRScheduler(
            optimizer=trainer.optim,
            init_lr=2e-6,
            max_lr=trainer_config.learning_rate,
            n_warmup_steps=trainer_config.warmup_steps,
        )
    elif trainer_config.lr_scheduler == &#34;linear&#34;:
        total_steps = int(
            trainer.num_epochs
            * len(list(trainer.dataloaders[&#34;train&#34;]))
            / (trainer_config.tgt_batch_size / trainer_config.batch_size)
        )
        init_lr = 2e-6
        trainer.scheduler = LinearWarmupDecayLRScheduler(
            trainer.optim,
            init_lr=init_lr,
            max_lr=trainer_config.learning_rate,
            n_warmup_steps=trainer_config.warmup_steps,
            total_steps=total_steps,
        )

    trainer.learning_rate_values.append(init_lr)
    trainer.grad_accum = trainer_config.tgt_batch_size &gt; trainer_config.batch_size

    if trainer.grad_accum:
        trainer.accumulation_steps = (
            trainer_config.tgt_batch_size // trainer.dataloaders[&#34;train&#34;].batch_size
        )
    else:
        trainer.accumulation_steps = 1

    return trainer</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.trainer.Trainer.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, inference: bool = False) ‑> Tuple[float, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Evaluate the model on the validation set and compute the average BLEU and ROUGE scores.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>inference</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True, evaluate the model in inference mode without using autocast. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, float]</code></dt>
<dd>The average BLEU and ROUGE scores on the validation set.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, inference: bool = False) -&gt; Tuple[float, float]:
    &#34;&#34;&#34;
    Evaluate the model on the validation set and compute the average BLEU and ROUGE scores.

    Args:
        inference (bool, optional): If True, evaluate the model in inference mode without using autocast. Defaults to False.

    Returns:
        Tuple[float, float]: The average BLEU and ROUGE scores on the validation set.
    &#34;&#34;&#34;
    self.model.eval()
    avg_bleu = 0
    avg_rouge = 0
    bleu = load_metric(&#34;bleu&#34;)
    rouge = load_metric(&#34;rouge&#34;)

    with torch.no_grad():
        for batch_idx, (src, tgt) in enumerate(self.dataloaders[&#34;val&#34;]):
            self.logger.info(
                f&#39;Evaluating batch {batch_idx+1}/{len(list(self.dataloaders[&#34;val&#34;]))}&#39;
            )
            tgt = tgt.type(torch.LongTensor)
            src = src.to(self.device)
            tgt = tgt.to(self.device)

            tgt_input = tgt[:-1, :]
            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = (
                self.translator.create_mask(src, tgt_input)
            )

            if inference:
                with torch.no_grad():
                    logits = self.model(
                        src,
                        tgt_input,
                        src_mask,
                        tgt_mask,
                        src_padding_mask,
                        tgt_padding_mask,
                    )
            else:
                with torch.autocast(
                    device_type=self.device,
                    dtype=torch.float16,
                    enabled=self.use_amp,
                ):
                    logits = self.model(
                        src,
                        tgt_input,
                        src_mask,
                        tgt_mask,
                        src_padding_mask,
                        tgt_padding_mask,
                    )

            predictions = torch.argmax(logits, dim=-1)
            predictions = predictions.T.cpu().numpy().tolist()
            targets = tgt_input.T.cpu().numpy().tolist()

            all_preds = self.tokenizer.decode_batch(predictions)
            all_targets = self.tokenizer.decode_batch(targets)

            bleu_score = bleu.compute(predictions=all_preds, references=all_targets)
            avg_bleu += bleu_score[&#34;bleu&#34;]

            rouge_score = rouge.compute(
                predictions=all_preds, references=all_targets
            )
            avg_rouge += rouge_score[&#34;rougeLsum&#34;]

    avg_bleu /= len(list(self.dataloaders[&#34;val&#34;]))
    avg_rouge /= len(list(self.dataloaders[&#34;val&#34;]))

    return avg_bleu, avg_rouge</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.Trainer.load_model"><code class="name flex">
<span>def <span class="ident">load_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Load the model from a checkpoint.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model(self):
    &#34;&#34;&#34;
    Load the model from a checkpoint.
    &#34;&#34;&#34;

    filepath = f&#34;./models/{self.run_id}/checkpoint.pt&#34;
    self.model = torch.jit.script(filepath)
    self.logger.info(f&#34;Model checkpoint have been loaded from {filepath}&#34;)</code></pre>
</details>
</dd>
<dt id="transformer.src.trainer.Trainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Train the model until convergence or early stopping.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self):
    &#34;&#34;&#34;
    Train the model until convergence or early stopping.
    &#34;&#34;&#34;
    try:
        for epoch in range(self.current_epoch, self.num_epochs + 1):
            start_time = perf_counter()
            train_loss = self._train_epoch()
            self.logger.info(
                f&#34;epoch {epoch} avg_training_loss: {round(train_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
            )

            start_time = perf_counter()
            test_loss = self._test_epoch()
            self.logger.info(
                f&#34;epoch {epoch} avg_test_loss: {round(test_loss, 3)} ({round(perf_counter()-start_time, 3)}s)&#34;
            )

            self.test_loss_values.append(test_loss)
            self.test_loss_steps.append(self.current_epoch * self.step_size)

            self.current_epoch += 1

            self._plot()

            early_stop_true = self.early_stopper.early_stop(epoch, test_loss)
            counter = self.early_stopper.counter

            if counter == 1:
                self._save_model(&#34;best_&#34;)

            if early_stop_true:
                self._save_model(&#34;last_&#34;)
                break
    except KeyboardInterrupt:
        self.logger.error(&#34;Training interrupted by user&#34;)
        self._save_model()
        sys.exit(0)

    self._save_model()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="transformer.src" href="index.html">transformer.src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="transformer.src.trainer.EarlyStopper" href="#transformer.src.trainer.EarlyStopper">EarlyStopper</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.trainer.EarlyStopper.early_stop" href="#transformer.src.trainer.EarlyStopper.early_stop">early_stop</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="transformer.src.trainer.InverseSquareRootLRScheduler" href="#transformer.src.trainer.InverseSquareRootLRScheduler">InverseSquareRootLRScheduler</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.trainer.InverseSquareRootLRScheduler.get_lr" href="#transformer.src.trainer.InverseSquareRootLRScheduler.get_lr">get_lr</a></code></li>
<li><code><a title="transformer.src.trainer.InverseSquareRootLRScheduler.step" href="#transformer.src.trainer.InverseSquareRootLRScheduler.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="transformer.src.trainer.LinearWarmupDecayLRScheduler" href="#transformer.src.trainer.LinearWarmupDecayLRScheduler">LinearWarmupDecayLRScheduler</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.trainer.LinearWarmupDecayLRScheduler.get_lr" href="#transformer.src.trainer.LinearWarmupDecayLRScheduler.get_lr">get_lr</a></code></li>
<li><code><a title="transformer.src.trainer.LinearWarmupDecayLRScheduler.step" href="#transformer.src.trainer.LinearWarmupDecayLRScheduler.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="transformer.src.trainer.Trainer" href="#transformer.src.trainer.Trainer">Trainer</a></code></h4>
<ul class="two-column">
<li><code><a title="transformer.src.trainer.Trainer.continue_training" href="#transformer.src.trainer.Trainer.continue_training">continue_training</a></code></li>
<li><code><a title="transformer.src.trainer.Trainer.evaluate" href="#transformer.src.trainer.Trainer.evaluate">evaluate</a></code></li>
<li><code><a title="transformer.src.trainer.Trainer.evaluate_checkpoint" href="#transformer.src.trainer.Trainer.evaluate_checkpoint">evaluate_checkpoint</a></code></li>
<li><code><a title="transformer.src.trainer.Trainer.load_model" href="#transformer.src.trainer.Trainer.load_model">load_model</a></code></li>
<li><code><a title="transformer.src.trainer.Trainer.new_instance" href="#transformer.src.trainer.Trainer.new_instance">new_instance</a></code></li>
<li><code><a title="transformer.src.trainer.Trainer.train" href="#transformer.src.trainer.Trainer.train">train</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>