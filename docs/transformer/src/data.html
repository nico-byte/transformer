<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>transformer.src.data API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>transformer.src.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import List, Tuple
from utils.logger import get_logger
import abc
import random
from src.pretrained_inference import mt_batch_inference
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchtext

torchtext.disable_torchtext_deprecation_warning()
from tokenizers import Tokenizer

from torchtext.datasets import Multi30k
from datasets import load_dataset
from utils.config import DataLoaderConfig, SharedConfig
from tokenizer.wordpiece_tokenizer import build_tokenizer as build_wordpiece_tokenizer
from tokenizer.unigram_tokenizer import build_tokenizer as build_unigram_tokenizer
from src.translate import check_device

# in case error occurs that it cant be imported by torch
torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = (
    torch.utils._import_utils.dill_available()
)


class BaseDataLoader(metaclass=abc.ABCMeta):
    &#34;&#34;&#34;
    Abstract base class for data loaders in the application. Provides common functionality for building datasets and dataloaders.

    ...
    
    Attributes:
        batch_size (int): The batch size for the dataloaders.
        num_workers (int): The number of worker processes to use for data loading.
        pin_memory (bool): Whether to use pinned memory for faster data transfer.
        drop_last (bool): Whether to drop the last incomplete batch.
        shuffle (bool): Whether to shuffle the data.
        tokenizer (Tokenizer): The tokenizer to use for encoding the input data.
        src_language (str): The source language.
        tgt_language (str): The target language.
        special_symbols (List[str]): A list of special symbols to use in the tokenizer.
        train_dataset (List[Tuple[str, str]]): The training dataset.
        val_dataset (List[Tuple[str, str]]): The validation dataset.
        test_dataset (List[Tuple[str, str]]): The test dataset.
        train_dataloader (DataLoader): The training dataloader.
        test_dataloader (DataLoader): The test dataloader.
        val_dataloader (DataLoader): The validation dataloader.
        logger (Logger): The logger for the data loader.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;Initializes the BaseDataLoader class.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application
        &#34;&#34;&#34;
        self.batch_size: int = dl_config.batch_size
        self.num_workers: int = dl_config.num_workers
        self.pin_memory: bool = dl_config.pin_memory
        self.drop_last: bool = dl_config.drop_last
        self.shuffle: bool = dl_config.shuffle
        self.tokenizer = None
        self.src_language: str = shared_config.src_language
        self.tgt_language: str = shared_config.tgt_language
        self.special_symbols: List[str] = shared_config.special_symbols

        self.train_dataset, self.val_dataset, self.test_dataset = [], [], []
        self.train_dataloader, self.test_dataloader, self.val_dataloader = (
            None,
            None,
            None,
        )

        self.logger = get_logger(&#34;DataLoader&#34;)

    @abc.abstractmethod
    def build_datasets(self):
        &#34;&#34;&#34;
        Abstract method for building datasets. Must be implemented by subclasses.
        &#34;&#34;&#34;
        pass

    def build_dataloaders(self):
        &#34;&#34;&#34;
        Build DataLoaders for the training, validation, and test datasets.
        &#34;&#34;&#34;
        self.train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )
        self.test_dataloader = DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )
        self.val_dataloader = DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )

    def collate_fn(
        self, batch: List[Tuple[str, str]]
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Collate function to process a batch of data.

        Args:
            batch (List[Tuple[str, str]]): Batch of source and target sequences.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Padded source and target batches as tensors.
        &#34;&#34;&#34;
        src_batch, tgt_batch = [], []
        for src_sample, tgt_sample in batch:
            encoded_src_sample = self.tokenizer.encode(src_sample)
            tensor_src_sample = torch.tensor(encoded_src_sample.ids)
            src_batch.append(tensor_src_sample)

            encoded_tgt_sample = self.tokenizer.encode(tgt_sample)
            tensor_tgt_sample = torch.tensor(encoded_tgt_sample.ids)
            tgt_batch.append(tensor_tgt_sample)

        src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=3)
        tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=3)

        return src_batch, tgt_batch

    def backtranslate_dataset(self):
        &#34;&#34;&#34;
        Backtranslate the dataset to augment the training data.

        This method takes the target sequences from the training dataset, backtranslates them using a machine translation model, and then adds the backtranslated sequences as new training examples.

        The resulting augmented dataset is then cleaned to remove any duplicate or empty sequences before being assigned back to the `self.train_dataset` attribute.
        &#34;&#34;&#34;
        tgt_dataset = [x[1] for x in self.train_dataset]

        device = check_device(&#34;cuda&#34;)

        backtrans_dataset = mt_batch_inference(tgt_dataset, device, 512, self.logger)

        backtrans_dataset_pairs = [
            [x, y] for x, y in zip(backtrans_dataset, tgt_dataset)
        ]

        new_dataset = self.train_dataset + backtrans_dataset_pairs
        self.train_dataset = self.clean_dataset(new_dataset)

    def clean_dataset(self, dataset: List[Tuple[str, str]]) -&gt; List[Tuple[str, str]]:
        &#34;&#34;&#34;
        Clean the dataset by removing duplicates and empty sequences.

        Args:
            dataset (List[Tuple[str, str]]): Dataset to be cleaned.

        Returns:
            List[Tuple[str, str]]: Cleaned dataset.
        &#34;&#34;&#34;

        src_dataset = [x[0] for x in dataset]
        tgt_dataset = [x[1] for x in dataset]

        unique_pairs = {}
        for x, y in zip(src_dataset, tgt_dataset):
            if (x and y) and (x not in unique_pairs):
                unique_pairs[x] = y

        clean_dataset = [[x, y] for x, y in unique_pairs.items()]
        self.logger.info(f&#34;Cleaned dataset: {len(clean_dataset)}&#34;)

        return clean_dataset

    def train_tokenizer(
        self, run_id: str, vocab_size: int, tokenizer: str = &#34;wordpiece&#34;
    ):
        &#34;&#34;&#34;
        This method loads the IWSLT2017 dataset, extracts the source and target language sequences, and then builds a tokenizer (either WordPiece or Unigram) using the training sequences. The trained tokenizer is then assigned to the `self.tokenizer` attribute.

        Args:
            run_id (str): A unique identifier for the current run.
            vocab_size (int): The desired vocabulary size for the tokenizer.
            tokenizer (str, optional): The type of tokenizer to use, either &#34;wordpiece&#34; or &#34;unigram&#34;. Defaults to &#34;wordpiece&#34;.
        &#34;&#34;&#34;
        dataset = load_dataset(
            &#34;iwslt2017&#34;,
            f&#34;iwslt2017-{self.src_language}-{self.tgt_language}&#34;,
            cache_dir=&#34;./.data/iwslt2017&#34;,
        )
        dataset = [
            (d[self.src_language], d[self.tgt_language])
            for d in dataset[&#34;train&#34;][&#34;translation&#34;]
        ]

        src_dataset = [x[0] for x in dataset]
        tgt_dataset = [x[1] for x in dataset]

        if tokenizer == &#34;wordpiece&#34;:
            self.tokenizer = build_wordpiece_tokenizer(
                run_id, src_dataset, tgt_dataset, vocab_size
            )
        else:
            self.tokenizer = build_unigram_tokenizer(
                run_id, src_dataset, tgt_dataset, vocab_size
            )

    @staticmethod
    def batch_iterator(dataset: List[Tuple[str, str]], batch_size: int = 1000):
        &#34;&#34;&#34;
        Generates batches of the given dataset.

        Args:
            dataset (Tuple[List[str, str]]): The dataset to be batched.
            batch_size (int, optional): The size of each batch. Defaults to 1000.

        Yields:
            Tuple[List[str, str]]: A batch of the dataset.
        &#34;&#34;&#34;
        for i in range(0, len(dataset), batch_size):
            yield dataset[i : i + batch_size]


class IWSLT2017DataLoader(BaseDataLoader):
    &#34;&#34;&#34;
    Class for providing the IWSLT2017 datasets, dataloaders and the tokenizer.
    
    ...
    
    Attributes:
        train_dataset (List[Tuple[str, str]]): Training dataset.
        test_dataset (List[Tuple[str, str]]): Test dataset.
        val_dataset (List[Tuple[str, str]]): Validation dataset.
        train_dataloader (DataLoader): Dataloader for the training dataset.
        test_dataloader (DataLoader): Dataloader for the test dataset.
        val_dataloader (DataLoader): Dataloader for the validation dataset.
        tokenizer (Tokenizer): Tokenizer for the training dataset.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;
        Initializes the IWSLT2017DataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader.
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application.
        &#34;&#34;&#34;
        super().__init__(dl_config, shared_config)

        self.dataset = load_dataset(
            &#34;iwslt2017&#34;,
            f&#34;iwslt2017-{self.src_language}-{self.tgt_language}&#34;,
            cache_dir=&#34;./.data/iwslt2017&#34;,
        )

        self.build_datasets()
        self.logger.info(&#34;Datasets have been loaded.&#34;)

    @classmethod
    def build_with_tokenizer(
        cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a pre-trained tokenizer.

        Args:
            dl_config (DataLoaderConfig): The data loader configuration.
            shared_config (SharedConfig): The shared configuration.
            tokenizer (str): The path to the pre-trained tokenizer file.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        dataloader.tokenizer = Tokenizer.from_file(tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    @classmethod
    def new_instance(
        cls,
        dl_config: DataLoaderConfig,
        shared_config: SharedConfig,
        tokenizer: str = &#34;wordpiece&#34;,
    ):
        &#34;&#34;&#34;
        Builds a new instance of the IWSLT2017DataLoader class with a pre-trained tokenizer.

        Args:
            dl_config (DataLoaderConfig): The data loader configuration.
            shared_config (SharedConfig): The shared configuration.
            tokenizer (str, optional): The path to the pre-trained tokenizer file. Defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        super().train_tokenizer(dataloader, shared_config.run_id, 3280, tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    def build_datasets(self):
        &#34;&#34;&#34;
        Builds the training, validation, and test datasets for the Multi30k dataset.

        The datasets are constructed by extracting the source and target language pairs from the
        &#34;translation&#34; field of the dataset. The training dataset is constructed from the &#34;train&#34;
        split, the test dataset is constructed from the &#34;test&#34; split, and the validation dataset
        is constructed from a random 5% sample of the training dataset.

        The first entry and length of each dataset are logged for debugging purposes.
        &#34;&#34;&#34;
        self.train_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;train&#34;][&#34;translation&#34;]
        ]
        self.test_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;test&#34;][&#34;translation&#34;]
        ]
        self.val_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;validation&#34;][&#34;translation&#34;]
        ]

        self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
        self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
        self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
        self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
        self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
        self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))


class Multi30kDataLoader(BaseDataLoader):
    &#34;&#34;&#34;
    Class for providing the Multi30K datasets, dataloaders and the tokenizer.
    
    ...
    
    Attributes:
        train_dataset (List[Tuple[str, str]]): Training dataset.
        test_dataset (List[Tuple[str, str]]): Test dataset.
        val_dataset (List[Tuple[str, str]]): Validation dataset.
        train_dataloader (DataLoader): Dataloader for the training dataset.
        test_dataloader (DataLoader): Dataloader for the test dataset.
        val_dataloader (DataLoader): Dataloader for the validation dataset.
        tokenizer (Tokenizer): Tokenizer for the training dataset.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;
        Initializes the Multi30KDataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader.
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application.
        &#34;&#34;&#34;
        super().__init__(dl_config, shared_config)

        self.build_datasets()
        self.logger.info(&#34;Datasets have benn loaded.&#34;)

    @classmethod
    def build_with_tokenizer(
        cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

        This class method is responsible for creating a new instance of the `Multi30kDataLoader` class, initializing the tokenizer, and building the necessary dataloaders for the Multi30k dataset.

        Args:
            dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
            shared_config (SharedConfig): A configuration object containing shared settings across the application.
            tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        dataloader.tokenizer = Tokenizer.from_file(tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    @classmethod
    def new_instance(
        cls,
        dl_config: DataLoaderConfig,
        shared_config: SharedConfig,
        back_translate: bool = True,
        tokenizer: str = &#34;wordpiece&#34;,
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

        Args:
            dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
            shared_config (SharedConfig): A configuration object containing shared settings across the application.
            tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        super().train_tokenizer(dataloader, shared_config.run_id, 1640, tokenizer)

        if back_translate:
            super().backtranslate_dataset(dataloader)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    def build_datasets(self):
        &#34;&#34;&#34;
        Builds the training, validation, and test datasets for the Multi30k dataset.
        This method is responsible for loading the Multi30k dataset, splitting it into training, validation, and test sets, and storing them as attributes of the `Multi30kDataLoader` instance.
        The training dataset is loaded from the &#39;train&#39; split of the Multi30k dataset, and 5% of the training samples are randomly selected to create the validation dataset. The remaining samples are kept in the training dataset.
        The test dataset is loaded from the &#39;valid&#39; split of the Multi30k dataset.
        The method also logs some debug information about the loaded datasets, including the first entry and the length of each dataset.
        &#34;&#34;&#34;
        self.train_dataset: List[str, str] = list(
            Multi30k(
                root=&#34;./.data/multi30k&#34;,
                split=&#34;train&#34;,
                language_pair=(self.src_language, self.tgt_language),
            )
        )

        self.test_dataset: List[str, str] = list(
            Multi30k(
                root=&#34;./.data/multi30k&#34;,
                split=&#34;valid&#34;,
                language_pair=(self.src_language, self.tgt_language),
            )
        )

        total_entries = len(self.train_dataset)
        num_test_entries = int(total_entries * 0.05)
        val_indices = random.sample(range(total_entries), num_test_entries)

        self.val_dataset: List[str, str] = [self.train_dataset[i] for i in val_indices]
        self.train_dataset = [
            entry for i, entry in enumerate(self.train_dataset) if i not in val_indices
        ]

        self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
        self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
        self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
        self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
        self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
        self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="transformer.src.data.BaseDataLoader"><code class="flex name class">
<span>class <span class="ident">BaseDataLoader</span></span>
<span>(</span><span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for data loaders in the application. Provides common functionality for building datasets and dataloaders.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The batch size for the dataloaders.</dd>
<dt><strong><code>num_workers</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of worker processes to use for data loading.</dd>
<dt><strong><code>pin_memory</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use pinned memory for faster data transfer.</dd>
<dt><strong><code>drop_last</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to drop the last incomplete batch.</dd>
<dt><strong><code>shuffle</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to shuffle the data.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>Tokenizer</code></dt>
<dd>The tokenizer to use for encoding the input data.</dd>
<dt><strong><code>src_language</code></strong> :&ensp;<code>str</code></dt>
<dd>The source language.</dd>
<dt><strong><code>tgt_language</code></strong> :&ensp;<code>str</code></dt>
<dd>The target language.</dd>
<dt><strong><code>special_symbols</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>A list of special symbols to use in the tokenizer.</dd>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>The training dataset.</dd>
<dt><strong><code>val_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>The validation dataset.</dd>
<dt><strong><code>test_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>The test dataset.</dd>
<dt><strong><code>train_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>The training dataloader.</dd>
<dt><strong><code>test_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>The test dataloader.</dd>
<dt><strong><code>val_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>The validation dataloader.</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>Logger</code></dt>
<dd>The logger for the data loader.</dd>
</dl>
<p>Initializes the BaseDataLoader class.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>DataLoaderConfig object that contains configuration settings for the data loader</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>SharedConfig object that contains shared configuration settings across the application</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDataLoader(metaclass=abc.ABCMeta):
    &#34;&#34;&#34;
    Abstract base class for data loaders in the application. Provides common functionality for building datasets and dataloaders.

    ...
    
    Attributes:
        batch_size (int): The batch size for the dataloaders.
        num_workers (int): The number of worker processes to use for data loading.
        pin_memory (bool): Whether to use pinned memory for faster data transfer.
        drop_last (bool): Whether to drop the last incomplete batch.
        shuffle (bool): Whether to shuffle the data.
        tokenizer (Tokenizer): The tokenizer to use for encoding the input data.
        src_language (str): The source language.
        tgt_language (str): The target language.
        special_symbols (List[str]): A list of special symbols to use in the tokenizer.
        train_dataset (List[Tuple[str, str]]): The training dataset.
        val_dataset (List[Tuple[str, str]]): The validation dataset.
        test_dataset (List[Tuple[str, str]]): The test dataset.
        train_dataloader (DataLoader): The training dataloader.
        test_dataloader (DataLoader): The test dataloader.
        val_dataloader (DataLoader): The validation dataloader.
        logger (Logger): The logger for the data loader.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;Initializes the BaseDataLoader class.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application
        &#34;&#34;&#34;
        self.batch_size: int = dl_config.batch_size
        self.num_workers: int = dl_config.num_workers
        self.pin_memory: bool = dl_config.pin_memory
        self.drop_last: bool = dl_config.drop_last
        self.shuffle: bool = dl_config.shuffle
        self.tokenizer = None
        self.src_language: str = shared_config.src_language
        self.tgt_language: str = shared_config.tgt_language
        self.special_symbols: List[str] = shared_config.special_symbols

        self.train_dataset, self.val_dataset, self.test_dataset = [], [], []
        self.train_dataloader, self.test_dataloader, self.val_dataloader = (
            None,
            None,
            None,
        )

        self.logger = get_logger(&#34;DataLoader&#34;)

    @abc.abstractmethod
    def build_datasets(self):
        &#34;&#34;&#34;
        Abstract method for building datasets. Must be implemented by subclasses.
        &#34;&#34;&#34;
        pass

    def build_dataloaders(self):
        &#34;&#34;&#34;
        Build DataLoaders for the training, validation, and test datasets.
        &#34;&#34;&#34;
        self.train_dataloader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )
        self.test_dataloader = DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )
        self.val_dataloader = DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=self.drop_last,
        )

    def collate_fn(
        self, batch: List[Tuple[str, str]]
    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;
        Collate function to process a batch of data.

        Args:
            batch (List[Tuple[str, str]]): Batch of source and target sequences.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: Padded source and target batches as tensors.
        &#34;&#34;&#34;
        src_batch, tgt_batch = [], []
        for src_sample, tgt_sample in batch:
            encoded_src_sample = self.tokenizer.encode(src_sample)
            tensor_src_sample = torch.tensor(encoded_src_sample.ids)
            src_batch.append(tensor_src_sample)

            encoded_tgt_sample = self.tokenizer.encode(tgt_sample)
            tensor_tgt_sample = torch.tensor(encoded_tgt_sample.ids)
            tgt_batch.append(tensor_tgt_sample)

        src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=3)
        tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=3)

        return src_batch, tgt_batch

    def backtranslate_dataset(self):
        &#34;&#34;&#34;
        Backtranslate the dataset to augment the training data.

        This method takes the target sequences from the training dataset, backtranslates them using a machine translation model, and then adds the backtranslated sequences as new training examples.

        The resulting augmented dataset is then cleaned to remove any duplicate or empty sequences before being assigned back to the `self.train_dataset` attribute.
        &#34;&#34;&#34;
        tgt_dataset = [x[1] for x in self.train_dataset]

        device = check_device(&#34;cuda&#34;)

        backtrans_dataset = mt_batch_inference(tgt_dataset, device, 512, self.logger)

        backtrans_dataset_pairs = [
            [x, y] for x, y in zip(backtrans_dataset, tgt_dataset)
        ]

        new_dataset = self.train_dataset + backtrans_dataset_pairs
        self.train_dataset = self.clean_dataset(new_dataset)

    def clean_dataset(self, dataset: List[Tuple[str, str]]) -&gt; List[Tuple[str, str]]:
        &#34;&#34;&#34;
        Clean the dataset by removing duplicates and empty sequences.

        Args:
            dataset (List[Tuple[str, str]]): Dataset to be cleaned.

        Returns:
            List[Tuple[str, str]]: Cleaned dataset.
        &#34;&#34;&#34;

        src_dataset = [x[0] for x in dataset]
        tgt_dataset = [x[1] for x in dataset]

        unique_pairs = {}
        for x, y in zip(src_dataset, tgt_dataset):
            if (x and y) and (x not in unique_pairs):
                unique_pairs[x] = y

        clean_dataset = [[x, y] for x, y in unique_pairs.items()]
        self.logger.info(f&#34;Cleaned dataset: {len(clean_dataset)}&#34;)

        return clean_dataset

    def train_tokenizer(
        self, run_id: str, vocab_size: int, tokenizer: str = &#34;wordpiece&#34;
    ):
        &#34;&#34;&#34;
        This method loads the IWSLT2017 dataset, extracts the source and target language sequences, and then builds a tokenizer (either WordPiece or Unigram) using the training sequences. The trained tokenizer is then assigned to the `self.tokenizer` attribute.

        Args:
            run_id (str): A unique identifier for the current run.
            vocab_size (int): The desired vocabulary size for the tokenizer.
            tokenizer (str, optional): The type of tokenizer to use, either &#34;wordpiece&#34; or &#34;unigram&#34;. Defaults to &#34;wordpiece&#34;.
        &#34;&#34;&#34;
        dataset = load_dataset(
            &#34;iwslt2017&#34;,
            f&#34;iwslt2017-{self.src_language}-{self.tgt_language}&#34;,
            cache_dir=&#34;./.data/iwslt2017&#34;,
        )
        dataset = [
            (d[self.src_language], d[self.tgt_language])
            for d in dataset[&#34;train&#34;][&#34;translation&#34;]
        ]

        src_dataset = [x[0] for x in dataset]
        tgt_dataset = [x[1] for x in dataset]

        if tokenizer == &#34;wordpiece&#34;:
            self.tokenizer = build_wordpiece_tokenizer(
                run_id, src_dataset, tgt_dataset, vocab_size
            )
        else:
            self.tokenizer = build_unigram_tokenizer(
                run_id, src_dataset, tgt_dataset, vocab_size
            )

    @staticmethod
    def batch_iterator(dataset: List[Tuple[str, str]], batch_size: int = 1000):
        &#34;&#34;&#34;
        Generates batches of the given dataset.

        Args:
            dataset (Tuple[List[str, str]]): The dataset to be batched.
            batch_size (int, optional): The size of each batch. Defaults to 1000.

        Yields:
            Tuple[List[str, str]]: A batch of the dataset.
        &#34;&#34;&#34;
        for i in range(0, len(dataset), batch_size):
            yield dataset[i : i + batch_size]</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="transformer.src.data.IWSLT2017DataLoader" href="#transformer.src.data.IWSLT2017DataLoader">IWSLT2017DataLoader</a></li>
<li><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="transformer.src.data.BaseDataLoader.batch_iterator"><code class="name flex">
<span>def <span class="ident">batch_iterator</span></span>(<span>dataset: List[Tuple[str, str]], batch_size: int = 1000)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates batches of the given dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Tuple[List[str, str]]</code></dt>
<dd>The dataset to be batched.</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The size of each batch. Defaults to 1000.</dd>
</dl>
<h2 id="yields">Yields</h2>
<dl>
<dt><code>Tuple[List[str, str]]</code></dt>
<dd>A batch of the dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def batch_iterator(dataset: List[Tuple[str, str]], batch_size: int = 1000):
    &#34;&#34;&#34;
    Generates batches of the given dataset.

    Args:
        dataset (Tuple[List[str, str]]): The dataset to be batched.
        batch_size (int, optional): The size of each batch. Defaults to 1000.

    Yields:
        Tuple[List[str, str]]: A batch of the dataset.
    &#34;&#34;&#34;
    for i in range(0, len(dataset), batch_size):
        yield dataset[i : i + batch_size]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.data.BaseDataLoader.backtranslate_dataset"><code class="name flex">
<span>def <span class="ident">backtranslate_dataset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Backtranslate the dataset to augment the training data.</p>
<p>This method takes the target sequences from the training dataset, backtranslates them using a machine translation model, and then adds the backtranslated sequences as new training examples.</p>
<p>The resulting augmented dataset is then cleaned to remove any duplicate or empty sequences before being assigned back to the <code>self.train_dataset</code> attribute.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backtranslate_dataset(self):
    &#34;&#34;&#34;
    Backtranslate the dataset to augment the training data.

    This method takes the target sequences from the training dataset, backtranslates them using a machine translation model, and then adds the backtranslated sequences as new training examples.

    The resulting augmented dataset is then cleaned to remove any duplicate or empty sequences before being assigned back to the `self.train_dataset` attribute.
    &#34;&#34;&#34;
    tgt_dataset = [x[1] for x in self.train_dataset]

    device = check_device(&#34;cuda&#34;)

    backtrans_dataset = mt_batch_inference(tgt_dataset, device, 512, self.logger)

    backtrans_dataset_pairs = [
        [x, y] for x, y in zip(backtrans_dataset, tgt_dataset)
    ]

    new_dataset = self.train_dataset + backtrans_dataset_pairs
    self.train_dataset = self.clean_dataset(new_dataset)</code></pre>
</details>
</dd>
<dt id="transformer.src.data.BaseDataLoader.build_dataloaders"><code class="name flex">
<span>def <span class="ident">build_dataloaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Build DataLoaders for the training, validation, and test datasets.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_dataloaders(self):
    &#34;&#34;&#34;
    Build DataLoaders for the training, validation, and test datasets.
    &#34;&#34;&#34;
    self.train_dataloader = DataLoader(
        self.train_dataset,
        batch_size=self.batch_size,
        collate_fn=self.collate_fn,
        shuffle=self.shuffle,
        num_workers=self.num_workers,
        pin_memory=self.pin_memory,
        drop_last=self.drop_last,
    )
    self.test_dataloader = DataLoader(
        self.test_dataset,
        batch_size=self.batch_size,
        collate_fn=self.collate_fn,
        shuffle=self.shuffle,
        num_workers=self.num_workers,
        pin_memory=self.pin_memory,
        drop_last=self.drop_last,
    )
    self.val_dataloader = DataLoader(
        self.val_dataset,
        batch_size=self.batch_size,
        collate_fn=self.collate_fn,
        shuffle=self.shuffle,
        num_workers=self.num_workers,
        pin_memory=self.pin_memory,
        drop_last=self.drop_last,
    )</code></pre>
</details>
</dd>
<dt id="transformer.src.data.BaseDataLoader.build_datasets"><code class="name flex">
<span>def <span class="ident">build_datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract method for building datasets. Must be implemented by subclasses.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def build_datasets(self):
    &#34;&#34;&#34;
    Abstract method for building datasets. Must be implemented by subclasses.
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="transformer.src.data.BaseDataLoader.clean_dataset"><code class="name flex">
<span>def <span class="ident">clean_dataset</span></span>(<span>self, dataset: List[Tuple[str, str]]) ‑> List[Tuple[str, str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Clean the dataset by removing duplicates and empty sequences.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Dataset to be cleaned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List[Tuple[str, str]]</code></dt>
<dd>Cleaned dataset.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_dataset(self, dataset: List[Tuple[str, str]]) -&gt; List[Tuple[str, str]]:
    &#34;&#34;&#34;
    Clean the dataset by removing duplicates and empty sequences.

    Args:
        dataset (List[Tuple[str, str]]): Dataset to be cleaned.

    Returns:
        List[Tuple[str, str]]: Cleaned dataset.
    &#34;&#34;&#34;

    src_dataset = [x[0] for x in dataset]
    tgt_dataset = [x[1] for x in dataset]

    unique_pairs = {}
    for x, y in zip(src_dataset, tgt_dataset):
        if (x and y) and (x not in unique_pairs):
            unique_pairs[x] = y

    clean_dataset = [[x, y] for x, y in unique_pairs.items()]
    self.logger.info(f&#34;Cleaned dataset: {len(clean_dataset)}&#34;)

    return clean_dataset</code></pre>
</details>
</dd>
<dt id="transformer.src.data.BaseDataLoader.collate_fn"><code class="name flex">
<span>def <span class="ident">collate_fn</span></span>(<span>self, batch: List[Tuple[str, str]]) ‑> Tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Collate function to process a batch of data.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Batch of source and target sequences.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[torch.Tensor, torch.Tensor]</code></dt>
<dd>Padded source and target batches as tensors.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def collate_fn(
    self, batch: List[Tuple[str, str]]
) -&gt; Tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;
    Collate function to process a batch of data.

    Args:
        batch (List[Tuple[str, str]]): Batch of source and target sequences.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: Padded source and target batches as tensors.
    &#34;&#34;&#34;
    src_batch, tgt_batch = [], []
    for src_sample, tgt_sample in batch:
        encoded_src_sample = self.tokenizer.encode(src_sample)
        tensor_src_sample = torch.tensor(encoded_src_sample.ids)
        src_batch.append(tensor_src_sample)

        encoded_tgt_sample = self.tokenizer.encode(tgt_sample)
        tensor_tgt_sample = torch.tensor(encoded_tgt_sample.ids)
        tgt_batch.append(tensor_tgt_sample)

    src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=3)
    tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=3)

    return src_batch, tgt_batch</code></pre>
</details>
</dd>
<dt id="transformer.src.data.BaseDataLoader.train_tokenizer"><code class="name flex">
<span>def <span class="ident">train_tokenizer</span></span>(<span>self, run_id: str, vocab_size: int, tokenizer: str = 'wordpiece')</span>
</code></dt>
<dd>
<div class="desc"><p>This method loads the IWSLT2017 dataset, extracts the source and target language sequences, and then builds a tokenizer (either WordPiece or Unigram) using the training sequences. The trained tokenizer is then assigned to the <code>self.tokenizer</code> attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>run_id</code></strong> :&ensp;<code>str</code></dt>
<dd>A unique identifier for the current run.</dd>
<dt><strong><code>vocab_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The desired vocabulary size for the tokenizer.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The type of tokenizer to use, either "wordpiece" or "unigram". Defaults to "wordpiece".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_tokenizer(
    self, run_id: str, vocab_size: int, tokenizer: str = &#34;wordpiece&#34;
):
    &#34;&#34;&#34;
    This method loads the IWSLT2017 dataset, extracts the source and target language sequences, and then builds a tokenizer (either WordPiece or Unigram) using the training sequences. The trained tokenizer is then assigned to the `self.tokenizer` attribute.

    Args:
        run_id (str): A unique identifier for the current run.
        vocab_size (int): The desired vocabulary size for the tokenizer.
        tokenizer (str, optional): The type of tokenizer to use, either &#34;wordpiece&#34; or &#34;unigram&#34;. Defaults to &#34;wordpiece&#34;.
    &#34;&#34;&#34;
    dataset = load_dataset(
        &#34;iwslt2017&#34;,
        f&#34;iwslt2017-{self.src_language}-{self.tgt_language}&#34;,
        cache_dir=&#34;./.data/iwslt2017&#34;,
    )
    dataset = [
        (d[self.src_language], d[self.tgt_language])
        for d in dataset[&#34;train&#34;][&#34;translation&#34;]
    ]

    src_dataset = [x[0] for x in dataset]
    tgt_dataset = [x[1] for x in dataset]

    if tokenizer == &#34;wordpiece&#34;:
        self.tokenizer = build_wordpiece_tokenizer(
            run_id, src_dataset, tgt_dataset, vocab_size
        )
    else:
        self.tokenizer = build_unigram_tokenizer(
            run_id, src_dataset, tgt_dataset, vocab_size
        )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="transformer.src.data.IWSLT2017DataLoader"><code class="flex name class">
<span>class <span class="ident">IWSLT2017DataLoader</span></span>
<span>(</span><span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for providing the IWSLT2017 datasets, dataloaders and the tokenizer.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Training dataset.</dd>
<dt><strong><code>test_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Test dataset.</dd>
<dt><strong><code>val_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Validation dataset.</dd>
<dt><strong><code>train_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the training dataset.</dd>
<dt><strong><code>test_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the test dataset.</dd>
<dt><strong><code>val_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the validation dataset.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>Tokenizer</code></dt>
<dd>Tokenizer for the training dataset.</dd>
</dl>
<p>Initializes the IWSLT2017DataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>DataLoaderConfig object that contains configuration settings for the data loader.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>SharedConfig object that contains shared configuration settings across the application.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IWSLT2017DataLoader(BaseDataLoader):
    &#34;&#34;&#34;
    Class for providing the IWSLT2017 datasets, dataloaders and the tokenizer.
    
    ...
    
    Attributes:
        train_dataset (List[Tuple[str, str]]): Training dataset.
        test_dataset (List[Tuple[str, str]]): Test dataset.
        val_dataset (List[Tuple[str, str]]): Validation dataset.
        train_dataloader (DataLoader): Dataloader for the training dataset.
        test_dataloader (DataLoader): Dataloader for the test dataset.
        val_dataloader (DataLoader): Dataloader for the validation dataset.
        tokenizer (Tokenizer): Tokenizer for the training dataset.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;
        Initializes the IWSLT2017DataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader.
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application.
        &#34;&#34;&#34;
        super().__init__(dl_config, shared_config)

        self.dataset = load_dataset(
            &#34;iwslt2017&#34;,
            f&#34;iwslt2017-{self.src_language}-{self.tgt_language}&#34;,
            cache_dir=&#34;./.data/iwslt2017&#34;,
        )

        self.build_datasets()
        self.logger.info(&#34;Datasets have been loaded.&#34;)

    @classmethod
    def build_with_tokenizer(
        cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a pre-trained tokenizer.

        Args:
            dl_config (DataLoaderConfig): The data loader configuration.
            shared_config (SharedConfig): The shared configuration.
            tokenizer (str): The path to the pre-trained tokenizer file.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        dataloader.tokenizer = Tokenizer.from_file(tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    @classmethod
    def new_instance(
        cls,
        dl_config: DataLoaderConfig,
        shared_config: SharedConfig,
        tokenizer: str = &#34;wordpiece&#34;,
    ):
        &#34;&#34;&#34;
        Builds a new instance of the IWSLT2017DataLoader class with a pre-trained tokenizer.

        Args:
            dl_config (DataLoaderConfig): The data loader configuration.
            shared_config (SharedConfig): The shared configuration.
            tokenizer (str, optional): The path to the pre-trained tokenizer file. Defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        super().train_tokenizer(dataloader, shared_config.run_id, 3280, tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    def build_datasets(self):
        &#34;&#34;&#34;
        Builds the training, validation, and test datasets for the Multi30k dataset.

        The datasets are constructed by extracting the source and target language pairs from the
        &#34;translation&#34; field of the dataset. The training dataset is constructed from the &#34;train&#34;
        split, the test dataset is constructed from the &#34;test&#34; split, and the validation dataset
        is constructed from a random 5% sample of the training dataset.

        The first entry and length of each dataset are logged for debugging purposes.
        &#34;&#34;&#34;
        self.train_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;train&#34;][&#34;translation&#34;]
        ]
        self.test_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;test&#34;][&#34;translation&#34;]
        ]
        self.val_dataset: List[str, str] = [
            (d[self.src_language], d[self.tgt_language])
            for d in self.dataset[&#34;validation&#34;][&#34;translation&#34;]
        ]

        self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
        self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
        self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
        self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
        self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
        self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="transformer.src.data.BaseDataLoader" href="#transformer.src.data.BaseDataLoader">BaseDataLoader</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="transformer.src.data.IWSLT2017DataLoader.build_with_tokenizer"><code class="name flex">
<span>def <span class="ident">build_with_tokenizer</span></span>(<span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig, tokenizer: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with a pre-trained tokenizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>The data loader configuration.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>The shared configuration.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the pre-trained tokenizer file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code></dt>
<dd>A new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with the specified tokenizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def build_with_tokenizer(
    cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
):
    &#34;&#34;&#34;
    Builds a new instance of the `Multi30kDataLoader` class with a pre-trained tokenizer.

    Args:
        dl_config (DataLoaderConfig): The data loader configuration.
        shared_config (SharedConfig): The shared configuration.
        tokenizer (str): The path to the pre-trained tokenizer file.

    Returns:
        Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
    &#34;&#34;&#34;
    dataloader = cls(dl_config, shared_config)

    dataloader.tokenizer = Tokenizer.from_file(tokenizer)

    super().build_dataloaders(dataloader)
    dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

    return dataloader</code></pre>
</details>
</dd>
<dt id="transformer.src.data.IWSLT2017DataLoader.new_instance"><code class="name flex">
<span>def <span class="ident">new_instance</span></span>(<span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig, tokenizer: str = 'wordpiece')</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new instance of the IWSLT2017DataLoader class with a pre-trained tokenizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>The data loader configuration.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>The shared configuration.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to the pre-trained tokenizer file. Defaults to "wordpiece".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code></dt>
<dd>A new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with the specified tokenizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def new_instance(
    cls,
    dl_config: DataLoaderConfig,
    shared_config: SharedConfig,
    tokenizer: str = &#34;wordpiece&#34;,
):
    &#34;&#34;&#34;
    Builds a new instance of the IWSLT2017DataLoader class with a pre-trained tokenizer.

    Args:
        dl_config (DataLoaderConfig): The data loader configuration.
        shared_config (SharedConfig): The shared configuration.
        tokenizer (str, optional): The path to the pre-trained tokenizer file. Defaults to &#34;wordpiece&#34;.

    Returns:
        Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified tokenizer.
    &#34;&#34;&#34;
    dataloader = cls(dl_config, shared_config)

    super().train_tokenizer(dataloader, shared_config.run_id, 3280, tokenizer)

    super().build_dataloaders(dataloader)
    dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

    return dataloader</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.data.IWSLT2017DataLoader.build_datasets"><code class="name flex">
<span>def <span class="ident">build_datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds the training, validation, and test datasets for the Multi30k dataset.</p>
<p>The datasets are constructed by extracting the source and target language pairs from the
"translation" field of the dataset. The training dataset is constructed from the "train"
split, the test dataset is constructed from the "test" split, and the validation dataset
is constructed from a random 5% sample of the training dataset.</p>
<p>The first entry and length of each dataset are logged for debugging purposes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_datasets(self):
    &#34;&#34;&#34;
    Builds the training, validation, and test datasets for the Multi30k dataset.

    The datasets are constructed by extracting the source and target language pairs from the
    &#34;translation&#34; field of the dataset. The training dataset is constructed from the &#34;train&#34;
    split, the test dataset is constructed from the &#34;test&#34; split, and the validation dataset
    is constructed from a random 5% sample of the training dataset.

    The first entry and length of each dataset are logged for debugging purposes.
    &#34;&#34;&#34;
    self.train_dataset: List[str, str] = [
        (d[self.src_language], d[self.tgt_language])
        for d in self.dataset[&#34;train&#34;][&#34;translation&#34;]
    ]
    self.test_dataset: List[str, str] = [
        (d[self.src_language], d[self.tgt_language])
        for d in self.dataset[&#34;test&#34;][&#34;translation&#34;]
    ]
    self.val_dataset: List[str, str] = [
        (d[self.src_language], d[self.tgt_language])
        for d in self.dataset[&#34;validation&#34;][&#34;translation&#34;]
    ]

    self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
    self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
    self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
    self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
    self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
    self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="transformer.src.data.BaseDataLoader" href="#transformer.src.data.BaseDataLoader">BaseDataLoader</a></b></code>:
<ul class="hlist">
<li><code><a title="transformer.src.data.BaseDataLoader.backtranslate_dataset" href="#transformer.src.data.BaseDataLoader.backtranslate_dataset">backtranslate_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.batch_iterator" href="#transformer.src.data.BaseDataLoader.batch_iterator">batch_iterator</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.build_dataloaders" href="#transformer.src.data.BaseDataLoader.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.clean_dataset" href="#transformer.src.data.BaseDataLoader.clean_dataset">clean_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.collate_fn" href="#transformer.src.data.BaseDataLoader.collate_fn">collate_fn</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.train_tokenizer" href="#transformer.src.data.BaseDataLoader.train_tokenizer">train_tokenizer</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="transformer.src.data.Multi30kDataLoader"><code class="flex name class">
<span>class <span class="ident">Multi30kDataLoader</span></span>
<span>(</span><span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for providing the Multi30K datasets, dataloaders and the tokenizer.</p>
<p>&hellip;</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Training dataset.</dd>
<dt><strong><code>test_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Test dataset.</dd>
<dt><strong><code>val_dataset</code></strong> :&ensp;<code>List[Tuple[str, str]]</code></dt>
<dd>Validation dataset.</dd>
<dt><strong><code>train_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the training dataset.</dd>
<dt><strong><code>test_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the test dataset.</dd>
<dt><strong><code>val_dataloader</code></strong> :&ensp;<code>DataLoader</code></dt>
<dd>Dataloader for the validation dataset.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>Tokenizer</code></dt>
<dd>Tokenizer for the training dataset.</dd>
</dl>
<p>Initializes the Multi30KDataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>DataLoaderConfig object that contains configuration settings for the data loader.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>SharedConfig object that contains shared configuration settings across the application.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Multi30kDataLoader(BaseDataLoader):
    &#34;&#34;&#34;
    Class for providing the Multi30K datasets, dataloaders and the tokenizer.
    
    ...
    
    Attributes:
        train_dataset (List[Tuple[str, str]]): Training dataset.
        test_dataset (List[Tuple[str, str]]): Test dataset.
        val_dataset (List[Tuple[str, str]]): Validation dataset.
        train_dataloader (DataLoader): Dataloader for the training dataset.
        test_dataloader (DataLoader): Dataloader for the test dataset.
        val_dataloader (DataLoader): Dataloader for the validation dataset.
        tokenizer (Tokenizer): Tokenizer for the training dataset.
    &#34;&#34;&#34;

    def __init__(self, dl_config: DataLoaderConfig, shared_config: SharedConfig):
        &#34;&#34;&#34;
        Initializes the Multi30KDataLoader class, which is responsible for loading and preparing the IWSLT2017 dataset for use in a machine learning model.

        Args:
            dl_config (DataLoaderConfig): DataLoaderConfig object that contains configuration settings for the data loader.
            shared_config (SharedConfig): SharedConfig object that contains shared configuration settings across the application.
        &#34;&#34;&#34;
        super().__init__(dl_config, shared_config)

        self.build_datasets()
        self.logger.info(&#34;Datasets have benn loaded.&#34;)

    @classmethod
    def build_with_tokenizer(
        cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

        This class method is responsible for creating a new instance of the `Multi30kDataLoader` class, initializing the tokenizer, and building the necessary dataloaders for the Multi30k dataset.

        Args:
            dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
            shared_config (SharedConfig): A configuration object containing shared settings across the application.
            tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        dataloader.tokenizer = Tokenizer.from_file(tokenizer)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    @classmethod
    def new_instance(
        cls,
        dl_config: DataLoaderConfig,
        shared_config: SharedConfig,
        back_translate: bool = True,
        tokenizer: str = &#34;wordpiece&#34;,
    ):
        &#34;&#34;&#34;
        Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

        Args:
            dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
            shared_config (SharedConfig): A configuration object containing shared settings across the application.
            tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

        Returns:
            Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
        &#34;&#34;&#34;
        dataloader = cls(dl_config, shared_config)

        super().train_tokenizer(dataloader, shared_config.run_id, 1640, tokenizer)

        if back_translate:
            super().backtranslate_dataset(dataloader)

        super().build_dataloaders(dataloader)
        dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

        return dataloader

    def build_datasets(self):
        &#34;&#34;&#34;
        Builds the training, validation, and test datasets for the Multi30k dataset.
        This method is responsible for loading the Multi30k dataset, splitting it into training, validation, and test sets, and storing them as attributes of the `Multi30kDataLoader` instance.
        The training dataset is loaded from the &#39;train&#39; split of the Multi30k dataset, and 5% of the training samples are randomly selected to create the validation dataset. The remaining samples are kept in the training dataset.
        The test dataset is loaded from the &#39;valid&#39; split of the Multi30k dataset.
        The method also logs some debug information about the loaded datasets, including the first entry and the length of each dataset.
        &#34;&#34;&#34;
        self.train_dataset: List[str, str] = list(
            Multi30k(
                root=&#34;./.data/multi30k&#34;,
                split=&#34;train&#34;,
                language_pair=(self.src_language, self.tgt_language),
            )
        )

        self.test_dataset: List[str, str] = list(
            Multi30k(
                root=&#34;./.data/multi30k&#34;,
                split=&#34;valid&#34;,
                language_pair=(self.src_language, self.tgt_language),
            )
        )

        total_entries = len(self.train_dataset)
        num_test_entries = int(total_entries * 0.05)
        val_indices = random.sample(range(total_entries), num_test_entries)

        self.val_dataset: List[str, str] = [self.train_dataset[i] for i in val_indices]
        self.train_dataset = [
            entry for i, entry in enumerate(self.train_dataset) if i not in val_indices
        ]

        self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
        self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
        self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
        self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
        self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
        self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="transformer.src.data.BaseDataLoader" href="#transformer.src.data.BaseDataLoader">BaseDataLoader</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="transformer.src.data.Multi30kDataLoader.build_with_tokenizer"><code class="name flex">
<span>def <span class="ident">build_with_tokenizer</span></span>(<span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig, tokenizer: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with a specified tokenizer.</p>
<p>This class method is responsible for creating a new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class, initializing the tokenizer, and building the necessary dataloaders for the Multi30k dataset.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>A configuration object containing settings for the data loader.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>A configuration object containing shared settings across the application.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the tokenizer to use, defaults to "wordpiece".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code></dt>
<dd>A new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with the specified configurations and tokenizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def build_with_tokenizer(
    cls, dl_config: DataLoaderConfig, shared_config: SharedConfig, tokenizer: str
):
    &#34;&#34;&#34;
    Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

    This class method is responsible for creating a new instance of the `Multi30kDataLoader` class, initializing the tokenizer, and building the necessary dataloaders for the Multi30k dataset.

    Args:
        dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
        shared_config (SharedConfig): A configuration object containing shared settings across the application.
        tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

    Returns:
        Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
    &#34;&#34;&#34;
    dataloader = cls(dl_config, shared_config)

    dataloader.tokenizer = Tokenizer.from_file(tokenizer)

    super().build_dataloaders(dataloader)
    dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

    return dataloader</code></pre>
</details>
</dd>
<dt id="transformer.src.data.Multi30kDataLoader.new_instance"><code class="name flex">
<span>def <span class="ident">new_instance</span></span>(<span>dl_config: utils.config.DataLoaderConfig, shared_config: utils.config.SharedConfig, back_translate: bool = True, tokenizer: str = 'wordpiece')</span>
</code></dt>
<dd>
<div class="desc"><p>Builds a new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with a specified tokenizer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dl_config</code></strong> :&ensp;<code>DataLoaderConfig</code></dt>
<dd>A configuration object containing settings for the data loader.</dd>
<dt><strong><code>shared_config</code></strong> :&ensp;<code>SharedConfig</code></dt>
<dd>A configuration object containing shared settings across the application.</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the tokenizer to use, defaults to "wordpiece".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code></dt>
<dd>A new instance of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> class with the specified configurations and tokenizer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def new_instance(
    cls,
    dl_config: DataLoaderConfig,
    shared_config: SharedConfig,
    back_translate: bool = True,
    tokenizer: str = &#34;wordpiece&#34;,
):
    &#34;&#34;&#34;
    Builds a new instance of the `Multi30kDataLoader` class with a specified tokenizer.

    Args:
        dl_config (DataLoaderConfig): A configuration object containing settings for the data loader.
        shared_config (SharedConfig): A configuration object containing shared settings across the application.
        tokenizer (str): The name of the tokenizer to use, defaults to &#34;wordpiece&#34;.

    Returns:
        Multi30kDataLoader: A new instance of the `Multi30kDataLoader` class with the specified configurations and tokenizer.
    &#34;&#34;&#34;
    dataloader = cls(dl_config, shared_config)

    super().train_tokenizer(dataloader, shared_config.run_id, 1640, tokenizer)

    if back_translate:
        super().backtranslate_dataset(dataloader)

    super().build_dataloaders(dataloader)
    dataloader.logger.info(&#34;Dataloaders have been built.&#34;)

    return dataloader</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="transformer.src.data.Multi30kDataLoader.build_datasets"><code class="name flex">
<span>def <span class="ident">build_datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Builds the training, validation, and test datasets for the Multi30k dataset.
This method is responsible for loading the Multi30k dataset, splitting it into training, validation, and test sets, and storing them as attributes of the <code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code> instance.
The training dataset is loaded from the 'train' split of the Multi30k dataset, and 5% of the training samples are randomly selected to create the validation dataset. The remaining samples are kept in the training dataset.
The test dataset is loaded from the 'valid' split of the Multi30k dataset.
The method also logs some debug information about the loaded datasets, including the first entry and the length of each dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_datasets(self):
    &#34;&#34;&#34;
    Builds the training, validation, and test datasets for the Multi30k dataset.
    This method is responsible for loading the Multi30k dataset, splitting it into training, validation, and test sets, and storing them as attributes of the `Multi30kDataLoader` instance.
    The training dataset is loaded from the &#39;train&#39; split of the Multi30k dataset, and 5% of the training samples are randomly selected to create the validation dataset. The remaining samples are kept in the training dataset.
    The test dataset is loaded from the &#39;valid&#39; split of the Multi30k dataset.
    The method also logs some debug information about the loaded datasets, including the first entry and the length of each dataset.
    &#34;&#34;&#34;
    self.train_dataset: List[str, str] = list(
        Multi30k(
            root=&#34;./.data/multi30k&#34;,
            split=&#34;train&#34;,
            language_pair=(self.src_language, self.tgt_language),
        )
    )

    self.test_dataset: List[str, str] = list(
        Multi30k(
            root=&#34;./.data/multi30k&#34;,
            split=&#34;valid&#34;,
            language_pair=(self.src_language, self.tgt_language),
        )
    )

    total_entries = len(self.train_dataset)
    num_test_entries = int(total_entries * 0.05)
    val_indices = random.sample(range(total_entries), num_test_entries)

    self.val_dataset: List[str, str] = [self.train_dataset[i] for i in val_indices]
    self.train_dataset = [
        entry for i, entry in enumerate(self.train_dataset) if i not in val_indices
    ]

    self.logger.debug(&#34;First Entry train dataset: %s&#34;, list(self.train_dataset[0]))
    self.logger.debug(&#34;Length train dataset: %f&#34;, len(self.train_dataset))
    self.logger.debug(&#34;First Entry test dataset: %s&#34;, list(self.test_dataset[0]))
    self.logger.debug(&#34;Length test dataset: %f&#34;, len(self.test_dataset))
    self.logger.debug(&#34;First Entry val dataset: %s&#34;, list(self.val_dataset[0]))
    self.logger.debug(&#34;Length val dataset: %f&#34;, len(self.val_dataset))</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="transformer.src.data.BaseDataLoader" href="#transformer.src.data.BaseDataLoader">BaseDataLoader</a></b></code>:
<ul class="hlist">
<li><code><a title="transformer.src.data.BaseDataLoader.backtranslate_dataset" href="#transformer.src.data.BaseDataLoader.backtranslate_dataset">backtranslate_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.batch_iterator" href="#transformer.src.data.BaseDataLoader.batch_iterator">batch_iterator</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.build_dataloaders" href="#transformer.src.data.BaseDataLoader.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.clean_dataset" href="#transformer.src.data.BaseDataLoader.clean_dataset">clean_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.collate_fn" href="#transformer.src.data.BaseDataLoader.collate_fn">collate_fn</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.train_tokenizer" href="#transformer.src.data.BaseDataLoader.train_tokenizer">train_tokenizer</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="transformer.src" href="index.html">transformer.src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="transformer.src.data.BaseDataLoader" href="#transformer.src.data.BaseDataLoader">BaseDataLoader</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.data.BaseDataLoader.backtranslate_dataset" href="#transformer.src.data.BaseDataLoader.backtranslate_dataset">backtranslate_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.batch_iterator" href="#transformer.src.data.BaseDataLoader.batch_iterator">batch_iterator</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.build_dataloaders" href="#transformer.src.data.BaseDataLoader.build_dataloaders">build_dataloaders</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.build_datasets" href="#transformer.src.data.BaseDataLoader.build_datasets">build_datasets</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.clean_dataset" href="#transformer.src.data.BaseDataLoader.clean_dataset">clean_dataset</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.collate_fn" href="#transformer.src.data.BaseDataLoader.collate_fn">collate_fn</a></code></li>
<li><code><a title="transformer.src.data.BaseDataLoader.train_tokenizer" href="#transformer.src.data.BaseDataLoader.train_tokenizer">train_tokenizer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="transformer.src.data.IWSLT2017DataLoader" href="#transformer.src.data.IWSLT2017DataLoader">IWSLT2017DataLoader</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.data.IWSLT2017DataLoader.build_datasets" href="#transformer.src.data.IWSLT2017DataLoader.build_datasets">build_datasets</a></code></li>
<li><code><a title="transformer.src.data.IWSLT2017DataLoader.build_with_tokenizer" href="#transformer.src.data.IWSLT2017DataLoader.build_with_tokenizer">build_with_tokenizer</a></code></li>
<li><code><a title="transformer.src.data.IWSLT2017DataLoader.new_instance" href="#transformer.src.data.IWSLT2017DataLoader.new_instance">new_instance</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="transformer.src.data.Multi30kDataLoader" href="#transformer.src.data.Multi30kDataLoader">Multi30kDataLoader</a></code></h4>
<ul class="">
<li><code><a title="transformer.src.data.Multi30kDataLoader.build_datasets" href="#transformer.src.data.Multi30kDataLoader.build_datasets">build_datasets</a></code></li>
<li><code><a title="transformer.src.data.Multi30kDataLoader.build_with_tokenizer" href="#transformer.src.data.Multi30kDataLoader.build_with_tokenizer">build_with_tokenizer</a></code></li>
<li><code><a title="transformer.src.data.Multi30kDataLoader.new_instance" href="#transformer.src.data.Multi30kDataLoader.new_instance">new_instance</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>