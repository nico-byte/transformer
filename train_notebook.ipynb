{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Project and import neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from src.data import IWSLT2017DataLoader, Multi30kDataLoader\n",
    "from utils.logger import get_logger\n",
    "from src.transformer import Seq2SeqTransformer\n",
    "from src.trainer import Trainer, EarlyStopper\n",
    "from utils.config import SharedConfig, DataLoaderConfig, TransformerConfig, TrainerConfig\n",
    "from src.processor import Processor\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Load Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = './configs/multi30k-small.yaml'\n",
    "run_id = 'multi30k-small'\n",
    "device = 'cuda'\n",
    "\n",
    "logger = get_logger(\"Main\")\n",
    "      \n",
    "if os.path.exists(f'./models/{run_id}/metrics'):\n",
    "      logger.error('Run ID already exists!')\n",
    "      sys.exit(1)\n",
    "else:\n",
    "      os.makedirs(f'./models/{run_id}/metrics')\n",
    "      \n",
    "with open(path_to_config) as stream:\n",
    "      config = yaml.safe_load(stream)\n",
    "\n",
    "shared_conf = SharedConfig(run_id=run_id)\n",
    "dl_conf = DataLoaderConfig(**config['dataloader'])\n",
    "\n",
    "if dl_conf.dataset == \"iwslt2017\":\n",
    "      dataloader = IWSLT2017DataLoader.new_instance(dl_conf, shared_conf)\n",
    "else:\n",
    "      dataloader = Multi30kDataLoader.new_instance(dl_conf, shared_conf)\n",
    "            \n",
    "train_dataloader = dataloader.train_dataloader\n",
    "test_dataloader = dataloader.test_dataloader\n",
    "val_dataloader = dataloader.val_dataloader\n",
    "tokenizer = dataloader.tokenizer\n",
    "val_dataset = dataloader.val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE, TGT_VOCAB_SIZE = tokenizer.get_vocab_size(), tokenizer.get_vocab_size()\n",
    "\n",
    "model_conf = TransformerConfig(\n",
    "      **config['transformer'], \n",
    "      src_vocab_size=SRC_VOCAB_SIZE, \n",
    "      tgt_vocab_size=TGT_VOCAB_SIZE\n",
    ")\n",
    "\n",
    "transformer = Seq2SeqTransformer(model_conf)\n",
    "translator = Processor.from_instance(transformer, tokenizer, device)\n",
    "\n",
    "trainer_conf = TrainerConfig(\n",
    "      **config['trainer'],\n",
    "      device=device, \n",
    "      batch_size=dl_conf.batch_size\n",
    ")\n",
    "summary(transformer, [(256, dl_conf.batch_size), (256, dl_conf.batch_size), \n",
    "                      (256, 256), (256, 256), \n",
    "                      (dl_conf.batch_size, 256), (dl_conf.batch_size, 256)], depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(warmup=17, patience=7, min_delta=0)\n",
    "\n",
    "trainer = Trainer.new_instance(transformer, translator, train_dataloader, test_dataloader, val_dataloader, \n",
    "                               tokenizer, early_stopper, trainer_conf, device, run_id)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only execute if no training was executed yet.\n",
    "shared_conf = SharedConfig()\n",
    "dl_conf = DataLoaderConfig()\n",
    "\n",
    "dataloader = IWSLT2017DataLoader(dl_conf, shared_conf)\n",
    "            \n",
    "val_dataset = dataloader.val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare translator, metrics and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric\n",
    "import torch\n",
    "# If one want to evaluate another model that was not trained before through this notebook - \n",
    "# specify here as a path to the model checkpoint and tokenizer path:\n",
    "# path_to_checkpoint = \"\"\n",
    "# path_to_tokenizer = \"\"\n",
    "# Don't forget to comment the paths below\n",
    "\n",
    "model_dir = f\"./models/{run_id}\"\n",
    "path_to_tokenizer = f\"{model_dir}/tokenizer.json\"\n",
    "\n",
    "if os.path.isfile(f\"{model_dir}/best_checkpoint_scripted.pt\"):\n",
    "    path_to_checkpoint = f\"{model_dir}/best_checkpoint_scripted.pt\"\n",
    "elif os.path.isfile(f\"{model_dir}/last_checkpoint_scripted.pt\"):\n",
    "    path_to_checkpoint = f\"{model_dir}/last_checkpoint_scripted.pt\"\n",
    "else:\n",
    "    path_to_checkpoint = f\"{model_dir}/checkpoint_scripted.pt\"\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "translator = Processor.from_checkpoint(model_checkpoint=path_to_checkpoint, \n",
    "                                             tokenizer=path_to_tokenizer, \n",
    "                                             device=device)\n",
    "      \n",
    "bleu = load_metric(\"bleu\")\n",
    "sacre_bleu = load_metric(\"sacrebleu\")\n",
    "rouge = load_metric(\"rouge\")\n",
    "meteor = load_metric(\"meteor\")\n",
    "\n",
    "outputs = []\n",
    "sources = [x[0] for x in val_dataset]\n",
    "targets = [x[1] for x in val_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, src in enumerate(sources):\n",
    "    output = translator.translate(src)\n",
    "            \n",
    "    outputs.append(output)\n",
    "            \n",
    "    print(f\"{idx+1}/{len(sources)}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "bleu_score = bleu.compute(predictions=outputs, references=targets)\n",
    "            \n",
    "sacre_bleu_score = sacre_bleu.compute(predictions=outputs, references=targets)\n",
    "                                \n",
    "rouge_score = rouge.compute(predictions=outputs, references=targets)\n",
    "      \n",
    "meteor_score = meteor.compute(predictions=outputs, references=targets)\n",
    "      \n",
    "metrics = {'bleu': bleu_score, \n",
    "           'sacre_bleu': sacre_bleu_score, \n",
    "           'rouge': rouge_score, \n",
    "           'meteor': meteor_score}\n",
    "      \n",
    "# Convert and write JSON object to file\n",
    "with open(f\"./{shared_conf.src_language}test-{shared_conf.tgt_language}-metrics.json\", \"x\") as outfile: \n",
    "    json.dump(metrics, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n\\nEvaluation: bleu_score - {bleu_score}\\nEvaluation: rouge_score - {rouge_score}\\nEvaluation: sacre_bleu_score - {sacre_bleu_score}\\nEvaluation: meteor_score - {meteor_score}')\n",
    "      \n",
    "TEST_SEQUENCE = \"The quick brown fox jumped over the lazy dog and then ran away quickly.\"\n",
    "output = translator.translate(TEST_SEQUENCE)\n",
    "      \n",
    "print(f'Input: {TEST_SEQUENCE}, Output: {output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neccessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from src.translate import check_device\n",
    "from utils.demo_model_config import ModelConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = check_device('cpu')\n",
    "\n",
    "# Initialize model configuration\n",
    "model_config = ModelConfig(device)\n",
    "\n",
    "# Set up Gradio theme\n",
    "theme = gr.themes.Default()\n",
    "\n",
    "en_examples = [\"The quick brown fox jumps over the lazy dog.\", \n",
    "                \"She sells seashells by the seashore.\", \n",
    "                \"Technology is rapidly changing the way we live and work.\", \n",
    "                \"Can you recommend a good restaurant nearby?\", \n",
    "                \"Despite the rain, they decided to go for a hike.\"]\n",
    "\n",
    "de_examples = [\"Die schnelle braune Katze sprang über den hohen Zaun.\", \n",
    "                \"Er spielte den ganzen Tag Videospiele.\", \n",
    "                \"Das neue Museum in der Stadt ist einen Besuch wert.\", \n",
    "                \"Kannst du mir helfen, dieses Problem zu lösen?\", \n",
    "                \"Obwohl sie müde war, arbeitete sie bis spät in die Nacht.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Dome Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t5_model_tab():\n",
    "    with gr.Tab(label=\"T5 Model\"):\n",
    "        with gr.Column():\n",
    "            with gr.Accordion(\"Debug Log\", open=True):\n",
    "                debug_log = gr.TextArea(label=\"\", lines=7, max_lines=12)\n",
    "\n",
    "            with gr.Group():\n",
    "                load_t5_btn = gr.Button(\"Load T5 model\")\n",
    "                load_t5_btn.click(fn=model_config.set_t5_model, outputs=[debug_log])\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    seed = gr.Textbox(label=\"English Sequence\", max_lines=2)\n",
    "                    model_id = gr.Textbox(value=\"t5\", visible=False)\n",
    "\n",
    "                with gr.Row():\n",
    "                    output = gr.Textbox(label=\"German Sequence\", max_lines=3)\n",
    "\n",
    "                with gr.Row():\n",
    "                    trns_btn = gr.Button(\"Translate\")\n",
    "                    trns_btn.click(fn=model_config.translate, inputs=[seed, model_id], outputs=[output])\n",
    "                    gr.ClearButton(components=[seed, output, debug_log])\n",
    "\n",
    "            with gr.Accordion(label=\"Examples\", open=True):\n",
    "                gr.Examples(examples=en_examples, inputs=[seed], label=\"English Sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model Demo Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model_tab():\n",
    "    with gr.Tab(label=\"Custom Model\"):\n",
    "        with gr.Column():\n",
    "            with gr.Accordion(\"Debug Log\", open=True):\n",
    "                debug_log = gr.TextArea(label=\"\", lines=7, max_lines=12)\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    model = gr.File(label=\"Model\", file_types=['.pt'], min_width=200)\n",
    "                    tokenizer = gr.File(label=\"Tokenizer\", file_types=['.json'], min_width=200)\n",
    "\n",
    "                with gr.Row():\n",
    "                    load_custom_btn = gr.Button(\"Load custom model\")\n",
    "                    load_custom_btn.click(fn=model_config.set_custom_model, inputs=[model, tokenizer], outputs=[debug_log])\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    seed = gr.Textbox(label=\"Input Sequence\", max_lines=2)\n",
    "                    model_id = gr.Textbox(value=\"custom\", visible=False)\n",
    "\n",
    "                with gr.Row():\n",
    "                    output = gr.Textbox(label=\"Output Sequence\", max_lines=3)\n",
    "\n",
    "                with gr.Row():\n",
    "                    trns_btn = gr.Button(\"Translate\")\n",
    "                    trns_btn.click(fn=model_config.translate, inputs=[seed, model_id], outputs=[output])\n",
    "                    gr.ClearButton(components=[seed, output, debug_log])\n",
    "\n",
    "            with gr.Accordion(label=\"Examples\", open=True):\n",
    "                gr.Examples(examples=en_examples, inputs=[seed], label=\"English Sequences\")\n",
    "                gr.Examples(examples=de_examples, inputs=[seed], label=\"German Sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(theme=theme) as demo:\n",
    "    header = gr.Markdown(\"# KI in den Life Sciences: Machine Translation Demo\")\n",
    "    line1 = gr.Markdown(\"by [Nico Fuchs](https://github.com/nico-byte) and [Matthias Laton](https://github.com/20DragonSlayer01)\")\n",
    "    line2 = gr.Markdown(\"---\")\n",
    "    line3 = gr.Markdown(\"### This demo uses a T5 model to translate English to German. You can also load your own model and tokenizer.\")\n",
    "\n",
    "    t5_model_tab()\n",
    "    custom_model_tab()\n",
    "\n",
    "# Launch the Gradio demo\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
