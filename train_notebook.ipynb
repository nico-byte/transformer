{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfuchs/transformer/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to ./.venv/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "import nltk\n",
    "from data import IWSLT2017DataLoader, Multi30kDataLoader\n",
    "from transformer import Seq2SeqTransformer\n",
    "from trainer import Trainer, EarlyStopper\n",
    "from config import SharedConfig, TokenizerConfig, DataLoaderConfig, TransformerConfig, TrainerConfig\n",
    "from translate import Processor\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "nltk.download('wordnet', download_dir='./.venv/share/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_language': 'de', 'tgt_language': 'en', 'src_tokenizer': functools.partial(<function _spacy_tokenize at 0x7727309040d0>, spacy=<spacy.lang.de.German object at 0x772723176290>), 'tgt_tokenizer': functools.partial(<function _spacy_tokenize at 0x7727309040d0>, spacy=<spacy.lang.en.English object at 0x77271055b820>)}\n",
      "{'special_symbols': ['<unk>', '<bos>', '<eos>', '<pad>']}\n",
      "{'dataset': 'multi30k', 'batch_size': 128, 'num_workers': 4, 'pin_memory': True, 'drop_last': False, 'shuffle': True}\n",
      "Creating DataLoaders:  |████████████████████████████████████████| 100% [3/3] in 1.3s (2.30/s) \n",
      "18643\n",
      "10596\n"
     ]
    }
   ],
   "source": [
    "path_to_config = './configs/multi30k-small.yaml'\n",
    "run_id = 'multi30k-small'\n",
    "device = 'cuda'\n",
    "      \n",
    "with open(path_to_config) as stream:\n",
    "      config = yaml.safe_load(stream)\n",
    "      \n",
    "tkn_conf = TokenizerConfig()\n",
    "print(tkn_conf.model_dump())\n",
    "      \n",
    "tokenizer = {\n",
    "      tkn_conf.src_language: tkn_conf.src_tokenizer,\n",
    "      tkn_conf.tgt_language: tkn_conf.tgt_tokenizer\n",
    "}\n",
    "\n",
    "\n",
    "shared_conf = SharedConfig()\n",
    "dl_conf = DataLoaderConfig(**config['dataloader'])\n",
    "print(shared_conf.model_dump())\n",
    "print(dl_conf.model_dump())\n",
    "\n",
    "if dl_conf.dataset == \"iwslt2017\":\n",
    "      dataloader = IWSLT2017DataLoader(dl_conf, tokenizer, tkn_conf, shared_conf)\n",
    "else:\n",
    "      dataloader = Multi30kDataLoader(dl_conf, tokenizer, tkn_conf, shared_conf)\n",
    "            \n",
    "vocab_transform, text_transform = dataloader.vocab_transform, dataloader.text_transform\n",
    "train_dataloader, test_dataloader, val_dataloader = dataloader.train_dataloader, dataloader.test_dataloader, dataloader.val_dataloader\n",
    "            \n",
    "SRC_VOCAB_SIZE = len(vocab_transform[tkn_conf.src_language].index2word)\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[tkn_conf.tgt_language].index2word)\n",
    "print(SRC_VOCAB_SIZE)\n",
    "print(TGT_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_encoder_layers': 3, 'num_decoder_layers': 3, 'emb_size': 512, 'nhead': 8, 'src_vocab_size': 18643, 'tgt_vocab_size': 10596, 'dim_feedforward': 1024, 'dropout': 0.1}\n",
      "{'learning_rate': 0.0001, 'num_epochs': 200, 'batch_size': 128, 'tgt_batch_size': 128, 'num_cycles': 6}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Seq2SeqTransformer                                 [256, 128, 10596]         --\n",
       "├─TokenEmbedding: 1-1                              [256, 128, 512]           --\n",
       "│    └─Embedding: 2-1                              [256, 128, 512]           9,545,216\n",
       "├─PositionalEncoding: 1-2                          [256, 128, 512]           --\n",
       "│    └─Dropout: 2-2                                [256, 128, 512]           --\n",
       "├─TokenEmbedding: 1-3                              [256, 128, 512]           --\n",
       "│    └─Embedding: 2-3                              [256, 128, 512]           5,425,152\n",
       "├─PositionalEncoding: 1-4                          [256, 128, 512]           --\n",
       "│    └─Dropout: 2-4                                [256, 128, 512]           --\n",
       "├─Transformer: 1-5                                 [256, 128, 512]           --\n",
       "│    └─TransformerEncoder: 2-5                     [256, 128, 512]           --\n",
       "│    │    └─ModuleList: 3-1                        --                        --\n",
       "│    │    │    └─TransformerEncoderLayer: 4-1      [256, 128, 512]           2,102,784\n",
       "│    │    │    └─TransformerEncoderLayer: 4-2      [256, 128, 512]           2,102,784\n",
       "│    │    │    └─TransformerEncoderLayer: 4-3      [256, 128, 512]           2,102,784\n",
       "│    │    └─LayerNorm: 3-2                         [256, 128, 512]           1,024\n",
       "│    └─TransformerDecoder: 2-6                     [256, 128, 512]           --\n",
       "│    │    └─ModuleList: 3-3                        --                        --\n",
       "│    │    │    └─TransformerDecoderLayer: 4-4      [256, 128, 512]           3,154,432\n",
       "│    │    │    └─TransformerDecoderLayer: 4-5      [256, 128, 512]           3,154,432\n",
       "│    │    │    └─TransformerDecoderLayer: 4-6      [256, 128, 512]           3,154,432\n",
       "│    │    └─LayerNorm: 3-4                         [256, 128, 512]           1,024\n",
       "├─Linear: 1-6                                      [256, 128, 10596]         5,435,748\n",
       "====================================================================================================\n",
       "Total params: 36,179,812\n",
       "Trainable params: 36,179,812\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.84\n",
       "====================================================================================================\n",
       "Input size (MB): 1.05\n",
       "Forward/backward pass size (MB): 7743.73\n",
       "Params size (MB): 106.90\n",
       "Estimated Total Size (MB): 7851.68\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conf = TransformerConfig(\n",
    "      **config['transformer'],\n",
    "      src_vocab_size=SRC_VOCAB_SIZE,\n",
    "      tgt_vocab_size=TGT_VOCAB_SIZE\n",
    ")\n",
    "print(model_conf.model_dump())\n",
    "\n",
    "transformer = Seq2SeqTransformer(model_conf)\n",
    "translator = Processor(transformer, device, shared_conf.special_symbols)\n",
    "\n",
    "trainer_conf = TrainerConfig(\n",
    "      **config['trainer'],\n",
    "      device=device\n",
    ")\n",
    "print(trainer_conf.model_dump())\n",
    "\n",
    "summary(transformer, [(256, dl_conf.batch_size), (256, dl_conf.batch_size), \n",
    "                      (256, 256), (256, 256), \n",
    "                      (dl_conf.batch_size, 256), (dl_conf.batch_size, 256)], depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0: epoch 1 avg_training_loss: 8.389605979124704\n",
      "on 0: epoch 1 avg_test_loss:     7.5691564083099365\n",
      "on 1: epoch 2 avg_training_loss: 6.934060313083507\n",
      "on 1: epoch 2 avg_test_loss:     6.02173924446106\n",
      "on 2: epoch 3 avg_training_loss: 5.571786730377762\n",
      "on 2: epoch 3 avg_test_loss:     5.0379029512405396\n",
      "on 3: epoch 4 avg_training_loss: 4.866412518201051\n",
      "on 3: epoch 4 avg_test_loss:     4.565712630748749\n",
      "on 4: epoch 5 avg_training_loss: 4.424694739006184\n",
      "on 4: epoch 5 avg_test_loss:     4.127844214439392\n",
      "on 5: epoch 6 avg_training_loss: 4.011514713366826\n",
      "on 5: epoch 6 avg_test_loss:     3.7310657501220703\n",
      "on 6: epoch 7 avg_training_loss: 3.6364861572230303\n",
      "on 6: epoch 7 avg_test_loss:     3.3684552013874054\n",
      "on 7: epoch 8 avg_training_loss: 3.292781162041205\n",
      "on 7: epoch 8 avg_test_loss:     3.0817970633506775\n",
      "on 8: epoch 9 avg_training_loss: 2.98689505899394\n",
      "on 8: epoch 9 avg_test_loss:     2.845249891281128\n",
      "on 9: epoch 10 avg_training_loss: 2.7174276610215506\n",
      "on 9: epoch 10 avg_test_loss:     2.6410519778728485\n",
      "on 10: epoch 11 avg_training_loss: 2.480599880218506\n",
      "on 10: epoch 11 avg_test_loss:     2.518152207136154\n",
      "on 11: epoch 12 avg_training_loss: 2.2672983739111157\n",
      "on 11: epoch 12 avg_test_loss:     2.3687995970249176\n",
      "on 12: epoch 13 avg_training_loss: 2.0785132624484874\n",
      "on 12: epoch 13 avg_test_loss:     2.274167001247406\n",
      "on 13: epoch 14 avg_training_loss: 1.9037194848060608\n",
      "on 13: epoch 14 avg_test_loss:     2.173334151506424\n",
      "on 14: epoch 15 avg_training_loss: 1.7465893620694126\n",
      "on 14: epoch 15 avg_test_loss:     2.1229825019836426\n",
      "on 15: epoch 16 avg_training_loss: 1.6062270761639983\n",
      "on 15: epoch 16 avg_test_loss:     2.0671829283237457\n",
      "on 16: epoch 17 avg_training_loss: 1.473497426620236\n",
      "on 16: epoch 17 avg_test_loss:     2.027258798480034\n",
      "on 17: epoch 18 avg_training_loss: 1.3548921826812956\n",
      "on 17: epoch 18 avg_test_loss:     1.9766268134117126\n",
      "on 18: epoch 19 avg_training_loss: 1.2429470651679568\n",
      "on 18: epoch 19 avg_test_loss:     1.9645357877016068\n",
      "on 19: epoch 20 avg_training_loss: 1.1465429834745549\n",
      "on 19: epoch 20 avg_test_loss:     1.9533152729272842\n",
      "on 20: epoch 21 avg_training_loss: 1.0638484885847126\n",
      "on 20: epoch 21 avg_test_loss:     1.9705564230680466\n",
      "on 21: epoch 22 avg_training_loss: 0.9887416409673514\n",
      "on 21: epoch 22 avg_test_loss:     1.952816590666771\n",
      "on 22: epoch 23 avg_training_loss: 0.9263308969912706\n",
      "on 22: epoch 23 avg_test_loss:     1.972534641623497\n",
      "on 23: epoch 24 avg_training_loss: 0.8696467741220085\n",
      "on 23: epoch 24 avg_test_loss:     1.9822456687688828\n",
      "on 24: epoch 25 avg_training_loss: 0.8188831331553282\n",
      "on 24: epoch 25 avg_test_loss:     1.9870602041482925\n",
      "Training: [====>!                                  ] (!) 24/200 [12%] in 5:45.3 (0.07/s) \n",
      "\n",
      "Evaluation: meteor_score - 0.6371716287745247\n",
      "Input: Eine Gruppe Pinguine steht vor einem Iglu und lacht sich tot ., Output:  A group of adults stand in front of an igloo laughing . \n"
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopper(patience=3, min_delta=0)\n",
    "\n",
    "trainer = Trainer(transformer, translator, train_dataloader, test_dataloader, val_dataloader, \n",
    "                  vocab_transform, early_stopper, trainer_conf, shared_conf, run_id, device)\n",
    "\n",
    "trainer.train()\n",
    "print(f'\\nEvaluation: meteor_score - {trainer.evaluate(tgt_language=tkn_conf.tgt_language)}')\n",
    "\n",
    "TEST_SEQUENCE = \"Eine Gruppe Pinguine steht vor einem Iglu und lacht sich tot .\"\n",
    "output = translator.translate(TEST_SEQUENCE, src_language=tkn_conf.src_language, \n",
    "                              tgt_language=tkn_conf.tgt_language, text_transform=text_transform, \n",
    "                              vocab_transform=vocab_transform, special_symbols=shared_conf.special_symbols)\n",
    "      \n",
    "print(f'Input: {TEST_SEQUENCE}, Output: {output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
